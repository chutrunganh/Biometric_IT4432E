{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset\n",
    "\n",
    "How we collect and buid our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain data for our face verification system, we use several methods:\n",
    "\n",
    "- Use pre-built datasets, in this case the LFW dataset\n",
    "\n",
    "- Use web scraping to collect data from the internet\n",
    "\n",
    "- Use video or image processing to collect data from real-world sources or capture data using webcam devices or video formats\n",
    "\n",
    "More details about each method are discussed in the following sections. Regardless of the method you choose to add data, create a folder named `data` inside the project folder and add all your data there.\n",
    "\n",
    "Note that this `data` folder is not committed to the repository, so run the following code to create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Pre build dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Access this [link](http://vis-www.cs.umass.edu/lfw/) to download the LFW dataset.\n",
    "\n",
    "Click on Download section -> All images as gzipped tar file -> lfw.tgz. Then move to the project workspace and extract the file using:\n",
    "\n",
    "```bash\n",
    "tar -xvzf lfw.tgz\n",
    "```\n",
    "\n",
    "After that, observe the `data` folder, you will see subfolders named after the person's name, each subfolder contains images of that person.\n",
    "\n",
    "\n",
    "![datasetView1](assets/images/datasetView1.png)\n",
    "\n",
    "\n",
    "The dataset is composed of 13233 images of 5749 people. The dataset is devicded into many subfolders, each subfolder contains images of a specific person. Each image is named as the person's name and a number, size of the image is 250x250 pixels and in JPEG format. The strucutre of the dataset is as follows:\n",
    "\n",
    "```plaintext\n",
    "lfw\n",
    "│\n",
    "|───person_1\n",
    "│   │   person_1_001.jpg\n",
    "│   │   person_1_002.jpg\n",
    "│   │   ...\n",
    "│\n",
    "|───person_2\n",
    "│   │   person_2_001.jpg\n",
    "│   │   person_2_002.jpg\n",
    "│   │   ...\n",
    "```\n",
    "\n",
    "Beside this dataset, we plan to use open source datasets provided by the University of Essex (face94, face95 and face96) if we have time and resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Collecting data from the internet using web scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the below script to collect images from the internet using the `bing_image_downloader` library. Specify the person's name as the keyword, it will search for images of that person and download them to a folder named after the person's name. Note that using this, you will need to manually verify the images to ensure they are of the correct person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bing_image_downloader in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (1.1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: urllib3 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: certifi in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bing_image_downloader\n",
    "%pip install --upgrade urllib3\n",
    "%pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Robert Downey Jr\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://www.newdvdreleasedates.com/images/profiles/robert-downey-jr.-12.jpg\n",
      "[!] Issue getting: https://www.newdvdreleasedates.com/images/profiles/robert-downey-jr.-12.jpg\n",
      "[!] Error:: HTTP Error 404: Not Found\n",
      "[%] Downloading Image #1 from https://m.media-amazon.com/images/M/MV5BNzg1MTUyNDYxOF5BMl5BanBnXkFtZTgwNTQ4MTE2MjE@._V1_.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://image.tmdb.org/t/p/original/5A7vGrVJcOLdfow1i9GoXN85Q16.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://people.com/thmb/TOOe_Ku31nqQx3va8hZa2EXAiU0=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(708x354:710x356)/Robert-Downey-Oppenheimer-071523-02-56152238e5064f8c954f688eb0b77cdc.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from http://br.web.img2.acsta.net/pictures/18/06/29/00/35/0101925.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[!!]Indexing page: 2\n",
      "\n",
      "[%] Indexed 95 Images on Page 2.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #5 from https://1.bp.blogspot.com/-mbsxDRgmiJE/W8OJWuAZbjI/AAAAAAAANYY/pnQNWsMQZuAQebg9JouM08K55UHULus9QCK4BGAYYCw/s1600/Robert%2BDowney%2BJr.%2Bgo%2Bprofile%2B1.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Will Smith\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://fr.web.img3.acsta.net/pictures/20/01/16/09/48/3201727.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://oneclimbs.com/wp-content/uploads/IMG_7836.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://cdn.wallpapersafari.com/37/86/8ZJoAd.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://api.time.com/wp-content/uploads/2016/06/will-smith-focus.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://www.attitude.co.uk/wp-content/uploads/sites/5/2022/10/29a44847-47b8-740a-f29d-d5f2b7aed68f.jpeg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Tom Cruise\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 20 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://pyxis.nymag.com/v1/imgs/4e5/1f7/a917c50e70a4c16bc35b9f0d8ce0352635-14-tom-cruise.rsquare.w700.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://www.etonline.com/sites/default/files/images/2022-07/Tom-Cruise-Getty-693134388.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://upload.wikimedia.org/wikipedia/commons/3/33/Tom_Cruise_by_Gage_Skidmore_2.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from http://images2.fanpop.com/images/photos/4100000/Tom-Cruise-tom-cruise-4181752-1274-1752.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://media.vanityfair.com/photos/627c70e7459259b942d73a39/master/w_2560%2Cc_limit/TGM-FF-125R2.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Miu Shiromine\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://s.kaskus.id/images/2021/08/01/10087324_202108011106080162.png\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://s.kaskus.id/images/2021/08/01/10087324_202108011105360498.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://s.kaskus.id/images/2021/08/01/10087324_202108011106510089.png\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://s359.kapook.com/pagebuilder/e95bda7a-3e0a-4c27-bf43-8a662e437239.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://s.kaskus.id/images/2021/08/01/10087324_202108011105480437.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Leonardo DiCaprio\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://image.tmdb.org/t/p/original/wo2hJpn04vbtmh0B9utCFdsQhxM.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from http://1.bp.blogspot.com/-k0RBUEOK1b8/UaNCKiCE6aI/AAAAAAAAAqs/pIGIwsmYv7Y/s1600/Leonardo+Dicaprio-Idea.png\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://ecofriendlywithemmy.com/wp-content/uploads/2021/03/Leonardo_DiCaprio-featured-image.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://prod-images.tcm.com/Master-Profile-Images/LeonardoDiCaprio.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://image.tmdb.org/t/p/original/aLUFp0zWpLVyIOgY0scIpuuKZLE.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Brad Pitt\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 20 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://media1.popsugar-assets.com/files/thumbor/8QCfbBsz_Tt7-Zavrc_q6rSrF9Q/356x1145:1857x2646/fit-in/2048xorig/filters:format_auto-!!-:strip_icc-!!-/2019/09/04/970/n/1922398/cc3fa7b15d70381d55bd82.88203803_/i/Brad-Pitt.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://flxt.tmsimg.com/assets/1366_v9_bc.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from http://fr.web.img2.acsta.net/pictures/20/02/10/10/37/1374948.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://factsfive.com/wp-content/uploads/2020/10/Brad-Pitt-Wiki-Bio-Age-Net-Worth-and-Other-Facts.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://kenh14cdn.com/2018/10/4/photo-1-15386424346791173725542.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Johnny Depp\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Johnny_Depp_Alice_Through_the_Looking_Glass_premiere.jpg/1200px-Johnny_Depp_Alice_Through_the_Looking_Glass_premiere.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from http://images6.fanpop.com/image/photos/33700000/Johnny-depp-johnny-depp-33763824-1600-1200.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://www.nme.com/wp-content/uploads/2023/05/Johnny-Depp-new-main-cannes.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from http://images6.fanpop.com/image/photos/32600000/Johnny-Depp-johnny-depp-32658587-1700-2534.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://m.media-amazon.com/images/M/MV5BOTBhMTI1NDQtYmU4Mi00MjYyLTk5MjEtZjllMDkxOWY3ZGRhXkEyXkFqcGdeQXVyNzI1NzMxNzM@._V1_.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Dwayne Johnson\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://3.bp.blogspot.com/-5ykK04jdq3M/XBPUt2rl9OI/AAAAAAAAPts/8HUbVU727Vw2JEn_iXpgWfa0mAL6iNYNQCLcBGAs/s1600/Dwayne+Johnson+go+proifle+1.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://www.dynamitenews.com/images/2020/09/19/dwayne-johnson-channelises-inner-black-adam-as-he-rips-off-gates-of-his-house/5f65b8445b997.jpeg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Dwayne_Johnson_2014_(cropped).jpg/640px-Dwayne_Johnson_2014_(cropped).jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://flxt.tmsimg.com/assets/235135_v9_bb.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://flxt.tmsimg.com/assets/235135_v9_bc.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Donald Trump\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 20 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Donald_Trump_official_portrait.jpg/1200px-Donald_Trump_official_portrait.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from http://upload.wikimedia.org/wikipedia/commons/e/ee/Donald_Trump_by_Gage_Skidmore.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://s4.scoopwhoop.com/anj2/5f58a7a918ac81421988a5e8/df5306de-7fd2-4f57-b142-84586b93d802.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://www.rawstory.com/media-library/donald-trump.jpg?id=33337164&amp;width=1200&amp;height=1066\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://media.cnn.com/api/v1/images/stellar/prod/180824143215-trump-family-portrait.jpg?q=w_4613,h_3280,x_0,y_0,c_fill/w_1280\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Elon Musk\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 5 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://ichef.bbci.co.uk/news/800/cpsprodpb/7727/production/_103330503_musk3.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://static1.businessinsider.com/image/59f8dc483e9d25db458b5dfc-2400/gettyimages-645671866.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from http://static1.businessinsider.com/image/5ab92326821646d7378b46e3-1781/56046ac3dd08958f038b45db.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://wp.technologyreview.com/wp-content/uploads/2020/02/elon-musk-ap-5.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://i2.cdn.turner.com/money/dam/assets/180907172129-musk-smoking-weed-1280x720.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n",
      "[%] Downloading Images to c:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\crawl_data\\Jensen Huang\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 20 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://history-computer.com/wp-content/uploads/2022/10/Jen-Hsun_Huang.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from https://www.cio.com/wp-content/uploads/2022/09/jensen-huang-3.jpg?quality=50&amp;strip=all\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://content.fortune.com/wp-content/uploads/2017/11/nvi12_a1.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://www.discoverwalks.com/blog/wp-content/uploads/2022/09/jensen_huang2.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://i.pinimg.com/736x/f7/05/c4/f705c497ac7fa3d8b5e5087a6c666bc2.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 5 images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bing_image_downloader import downloader\n",
    "from PIL import Image\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# List of persons\n",
    "persons = [\n",
    "    \"Robert Downey Jr\", \"Will Smith\", \"Tom Cruise\", \"Miu Shiromine\",\n",
    "    \"Leonardo DiCaprio\", \"Brad Pitt\", \"Johnny Depp\", \"Dwayne Johnson\",\n",
    "    \"Donald Trump\", \"Elon Musk\", \"Jensen Huang\"\n",
    "]\n",
    "\n",
    "# Download images for each person\n",
    "for person in persons:\n",
    "\n",
    "    try:\n",
    "    \n",
    "        # Find and download images of that person\n",
    "        downloader.download(person, limit=5, output_dir='crawl_data', \n",
    "                            adult_filter_off=True, force_replace=False, timeout=60)\n",
    "        \n",
    "\n",
    "        person_path = os.path.join('crawl_data', person)\n",
    "\n",
    "        # Rename downloaded images\n",
    "        for idx, filename in enumerate(os.listdir(person_path)):\n",
    "            try: \n",
    "                if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                    new_filename = f\"{person.replace(\" \",\"_\")}_{idx+1}.jpg\"\n",
    "                    old_file_path = os.path.join(person_path, filename)\n",
    "                    new_file_path = os.path.join(person_path, new_filename)\n",
    "                    \n",
    "                    # Rename the file\n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "                    \n",
    "                    # Crop and resize the image (in case image is square, resize to 250x250)\n",
    "                    # In case image is not square, crop the center square and resize to 250x250\n",
    "                    with Image.open(new_file_path) as img:\n",
    "                        width, height = img.size\n",
    "                        min_dim = min(width, height)\n",
    "                        left = (width - min_dim) / 2\n",
    "                        top = (height - min_dim) / 2\n",
    "                        right = (width + min_dim) / 2\n",
    "                        bottom = (height + min_dim) / 2\n",
    "                        img = img.crop((left, top, right, bottom))\n",
    "                        img = img.resize((250, 250))\n",
    "\n",
    "                        # Convert to RGB if necessary\n",
    "                        if img.mode == 'RGBA':\n",
    "                            img = img.convert('RGB')\n",
    "                            \n",
    "                        img.save(new_file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {filename} for {person}: {e}\")\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading images for {person}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawling data is saves to `crawl_data` folder. You might want to check these images manually again to ensure the data quality, then you can drag and drop these image folder in the main data folder `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Collecting data from the real world using webcam devices or video format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input Path\n",
    "video_path = '/home/jawabreh/Desktop/face_scan.MOV'\n",
    "\n",
    "# Output Path\n",
    "output_dir = '/home/jawabreh/Desktop/face-recognition/training/person_1'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Create a VideoCapture object to read the input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the total number of frames in the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Calculate the frame interval to capture for 150 images\n",
    "frame_interval = total_frames // 1000 # change this number according to your needs \n",
    "\n",
    "# Set the initial frame counter to 0\n",
    "frame_counter = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Check if this is the frame to capture\n",
    "    if frame_counter % frame_interval == 0 and frame_counter // frame_interval < 1000:\n",
    "        # Save the frame as a JPEG image\n",
    "        output_path = os.path.join(output_dir, f'{frame_counter//frame_interval + 1:03}.jpg')\n",
    "        cv2.imwrite(output_path, frame)\n",
    "    \n",
    "    # Increment the frame counter\n",
    "    frame_counter += 1\n",
    "    \n",
    "    if frame_counter >= total_frames:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "print(\"\\n\\nDONE\\n\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An alternative way to capture images from a webcam is to use the following script, using p to capture an image and q to quit the program instead of using a video**:\n",
    "\n",
    "```python\n",
    "# Defined the Camera ID to use\n",
    "CAM_ID = 3 # Establishing the connection with the IR camera\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Function to save the captured image to the specified folder\n",
    "def save_image(image, folder_path, img_name):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    cv2.imwrite(img_path, image)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(CAM_ID)\n",
    "\n",
    "\n",
    "# Get the name of the person to store in training data\n",
    "name = input(\"Name of the person to store in training data: \")\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face collect for as training data, press `p` to cpture, `q` for quit', frame)\n",
    "    \n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('p'):\n",
    "        # Save the frame to './data' folder\n",
    "        save_path = os.path.join('data', name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        save_image(frame, save_path, str(uuid.uuid1()) + \".jpg\")\n",
    "        print(\"Image saved to\", save_path)\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "The above code will prompt usr to enter name, then it will capture images from the webcam when press `p` and save them to a folder named after the person's name inside the `data` folder.\n",
    "\n",
    "\n",
    "- CAM_ID = 0 for laptop normal webcam\n",
    "- CAM_ID = 2 for laptop IR webcam\n",
    "- CAM_ID = 4 for external webcam\n",
    "\n",
    "\n",
    "***Depend on each devices, these number can be different. Try out all number start from 0 and see which one is the correct one on your device.***\n",
    "\n",
    "\n",
    "Set up camera:\n",
    "\n",
    "**IR webcam:**\n",
    "\n",
    "What is IR webcam? https://fptshop.com.vn/tin-tuc/danh-gia/ir-camera-la-gi-153147\n",
    "\n",
    "\n",
    "**External webcam**\n",
    "\n",
    "Since the resolution of the laptop webcam is not good, we will use an external webcam like from a mobile phone. To connect the webcam to the laptop:\n",
    "\n",
    "1. Download the DroidCam app on your phone, also the DroidCam client on your laptop: https://www.dev47apps.com/\n",
    "2. Set up as the instruction on the website. With Linux:\n",
    "\n",
    "```bash\n",
    "wget -O droidcam_latest.zip https://files.dev47apps.net/linux/droidcam_2.1.3.zip\n",
    "unzip droidcam_latest.zip -d droidcam\n",
    "cd droidcam && sudo ./install-client\n",
    "sudo apt install libappindicator3-1\n",
    "\n",
    "# Fix missing video device\n",
    "sudo apt install linux-headers-`uname -r` gcc make\n",
    "sudo ./install-video\n",
    "```\n",
    "3. Open both DroidCam on phone and DroidCam client on laptop, connect the phone to the laptop via USB or Wifi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Currently, we are just testing on the LWF dataset, but use the second and third methods can be use to enhace the diversity or specific data we need for our face verification system. Most of iamges inside the LWF dataset are people in Western countries, so we can use the second and third methods to collect images of people from our specific region or country. This will help the model perfrom better on our target market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Reduce amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase you want to run this notebook on your machine, but the size of LWF dataset overwhelms your machine, you can run the following code delete some random subfolders from the dataset, only keeping as you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_dir = os.path.join('.', 'lfw') # REPLACE WITH YOUR PATH to the LFW dataset (after extracting the zip file)\n",
    "# In my case, it located in the same directory as this notebook\n",
    "\n",
    "# We do not wnat to modify/delete directly the original lfw dataset, \n",
    "# so we will copy it to the `data` directory we created before and process\n",
    "data_dir = os.path.join('.', 'data')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the data folder if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Copy the content of the lfw folder to the data folder\n",
    "for item in os.listdir(lfw_dir):\n",
    "    s = os.path.join(lfw_dir, item)\n",
    "    d = os.path.join(data_dir, item)\n",
    "    if os.path.isdir(s):\n",
    "        if not os.path.exists(d):\n",
    "            shutil.copytree(s, d)\n",
    "        else:\n",
    "            for sub_item in os.listdir(s):\n",
    "                sub_s = os.path.join(s, sub_item)\n",
    "                sub_d = os.path.join(d, sub_item)\n",
    "                if os.path.isdir(sub_s):\n",
    "                    shutil.copytree(sub_s, sub_d, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(sub_s, sub_d)\n",
    "    else:\n",
    "        shutil.copy2(s, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 20 subfolders and deleted the rest.\n"
     ]
    }
   ],
   "source": [
    "# Perform randomly delete subfolders to reudce the size of the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "# Get a list of all subfolders\n",
    "subfolders = [f.path for f in os.scandir(data_dir) if f.is_dir()]\n",
    "\n",
    "# Shuffle the list of subfolders\n",
    "random.shuffle(subfolders)\n",
    "\n",
    "# Keep only 20 subfolders\n",
    "subfolders_to_keep = subfolders[:20]\n",
    "\n",
    "# Delete the remaining subfolders\n",
    "for subfolder in subfolders[20:]:\n",
    "    for root, dirs, files in os.walk(subfolder, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "    os.rmdir(subfolder)\n",
    "\n",
    "print(f\"Kept {len(subfolders_to_keep)} subfolders and deleted the rest.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Arrangement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create augmented images and store them in the same folder as the original images, which is the `data` folder.\n",
    "\n",
    "Inside the `data` folder, there are many subfolders, each containing images of a person. The number of images in each subfolder varies. Count the number of images in each subfolder. If a subfolder has many images (high density), apply only a few augmentation operations to each image. Otherwise (low density), apply more augmentations to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting numpy>=1.21.2 (from opencv-python)\n",
      "  Using cached numpy-2.2.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached numpy-2.2.0-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-2.2.0 opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Using cached albumentations-1.4.23-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from albumentations) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from albumentations) (1.14.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting albucore==0.0.21 (from albumentations)\n",
      "  Using cached albucore-0.0.21-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting eval-type-backport (from albumentations)\n",
      "  Using cached eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from albucore==0.0.21->albumentations) (3.11.2)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from albucore==0.0.21->albumentations) (6.2.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\chu trung anh\\desktop\\biometric_it4432e\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
      "Using cached albumentations-1.4.23-py3-none-any.whl (269 kB)\n",
      "Using cached albucore-0.0.21-py3-none-any.whl (12 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Using cached eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: opencv-python-headless, eval-type-backport, annotated-types, pydantic, albucore, albumentations\n",
      "Successfully installed albucore-0.0.21 albumentations-1.4.23 annotated-types-0.7.0 eval-type-backport-0.2.0 opencv-python-headless-4.10.0.84 pydantic-2.10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "# For argumetation operations\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    RandomBrightnessContrast,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    Rotate,\n",
    "    ShiftScaleRotate,\n",
    "    HueSaturationValue,\n",
    "    GaussianBlur,\n",
    "    GaussNoise,\n",
    "    ElasticTransform,\n",
    "    GridDistortion,\n",
    "    CLAHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of images inside each subfolder, if there are quite many (high density)\n",
    "# we will only a few argumetation operations to each image, othersise,(low density) we will apply more augmentations to each image.\n",
    "def count_images(folder):\n",
    "    \"\"\"Count number of image files in folder\"\"\"\n",
    "    return len([f for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "\n",
    "# Based on the number of images in a folder (density), we will decide the how many augmentations we will apply to each image.\n",
    "def create_augmentations(density='high'):\n",
    "    \"\"\"Create list of augmentations based on density\"\"\"\n",
    "    if density == 'high':\n",
    "        # Limited augmentations with high density\n",
    "        augmentations = [\n",
    "            RandomBrightnessContrast(p=1.0),\n",
    "            HorizontalFlip(p=1.0),\n",
    "        ]\n",
    "    else:\n",
    "        # More augmentations: Brightless,Horizontal Flip, Rotate, Zoom, Vary color, ...\n",
    "        augmentations = [\n",
    "            RandomBrightnessContrast(p=1.0),\n",
    "            HorizontalFlip(p=1.0),\n",
    "            Rotate(limit=60, p=1.0),\n",
    "            ShiftScaleRotate(\n",
    "                shift_limit=0.3,\n",
    "                scale_limit=0.3,\n",
    "                rotate_limit=20,\n",
    "                p=1.0\n",
    "            ),\n",
    "            HueSaturationValue(p=1.0),\n",
    "        ]\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "# Augment images in a folder using predefined density transformations\n",
    "def augment_folder(folder_path, density):\n",
    "    \"\"\"Augment images in folder using predefined density transformations\"\"\"\n",
    "    augmentations = create_augmentations(density)\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Randomly decide how many images to augment (both in the number and which images)\n",
    "    num_to_augment = random.randint(1, len(os.listdir(folder_path)))\n",
    "    images_to_augment = random.sample(os.listdir(folder_path), num_to_augment)\n",
    "    \n",
    "    for img_name in images_to_augment:\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            for aug in augmentations:\n",
    "                # Apply each augmentation separately\n",
    "                augmented = aug(image=img)\n",
    "                augmented_image = augmented[\"image\"]\n",
    "                \n",
    "                filename, ext = os.path.splitext(img_name)\n",
    "                aug_name = aug.__class__.__name__\n",
    "                aug_filename = f\"{filename}_{aug_name}{ext}\"\n",
    "                aug_path = os.path.join(folder_path, aug_filename)\n",
    "                \n",
    "                aug_bgr = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(aug_path, aug_bgr) # Store  name of original image + augmentation type\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "            \n",
    "    print(f\"Augmentation completed for folder: {folder_path} with density: {density}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation completed for folder: data\\Alistair_MacDonald with density: low\n",
      "Augmentation completed for folder: data\\Andrew_Wetzler with density: low\n",
      "Augmentation completed for folder: data\\Bruce_Gebhardt with density: low\n",
      "Augmentation completed for folder: data\\Charles_Schumer with density: low\n",
      "Augmentation completed for folder: data\\Connie_Freydell with density: low\n",
      "Augmentation completed for folder: data\\David_Duval with density: low\n",
      "Augmentation completed for folder: data\\Gianna_Angelopoulos-Daskalaki with density: low\n",
      "Augmentation completed for folder: data\\Hank_Azaria with density: low\n",
      "Augmentation completed for folder: data\\Hasan_Wirayuda with density: low\n",
      "Augmentation completed for folder: data\\Javier_Bardem with density: low\n",
      "Augmentation completed for folder: data\\Jeff_George with density: low\n",
      "Augmentation completed for folder: data\\Jorge_Castaneda with density: low\n",
      "Augmentation completed for folder: data\\Leslie_Moonves with density: low\n",
      "Augmentation completed for folder: data\\Marc_Shaiman with density: low\n",
      "Augmentation completed for folder: data\\Melissa_Etheridge with density: low\n",
      "Augmentation completed for folder: data\\Michael_Jasny with density: low\n",
      "Augmentation completed for folder: data\\Rachel_Hunter with density: high\n",
      "Augmentation completed for folder: data\\Rodrigo_de_la_Cerna with density: low\n",
      "Augmentation completed for folder: data\\Rogelio_Ramos with density: low\n",
      "Augmentation completed for folder: data\\Steve_Blankenship with density: low\n"
     ]
    }
   ],
   "source": [
    "data_directory = 'data'\n",
    "DENSITY_THRESHOLD = 3 # If inside as folder, there are more than 3 images, we consider it as high density, otherwise, low density\n",
    "\n",
    "# fProcessing each subfolder in the data directory\n",
    "for subfolder in os.listdir(data_directory):\n",
    "    folder_path = os.path.join(data_directory, subfolder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        num_images = count_images(folder_path)\n",
    "        if num_images > DENSITY_THRESHOLD:\n",
    "            density = 'high'\n",
    "        else:\n",
    "            density = 'low'\n",
    "        augment_folder(folder_path, density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Complete data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add your own data as you liek, just put all of them inside the `data` folder. The structure of the data folder should be as follows:\n",
    "\n",
    "```plaintext\n",
    "data\n",
    "│\n",
    "|───person_1\n",
    "│   │   person_1_001.jpg\n",
    "│   │   person_1_002.jpg\n",
    "│   │   ...\n",
    "│\n",
    "|───person_2\n",
    "│   │   person_2_001.jpg\n",
    "│   │   person_2_002.jpg\n",
    "│   │   ...\n",
    "```\n",
    "\n",
    "Now, we already have the dataset, we continue to preprocessing these data for the training process. \n",
    "\n",
    "- For the first pipleine (Facenet + SVM), check the data process inside the `Pipeline1 DataPreprocessing.ipynb` notebook, then the training pharse inside the `SVM_Classifier.ipynb` notebook.\n",
    "\n",
    "- For the second pipeline (Siamese Architecture + L1 distance), the preprocessing data process is inside the `Pipeline2 DataPreprocessing.ipynb` notebook, then the training pharse inside the `Siamese_Network.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for face verification app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies and CAM ID setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed training some model and store those models to `model_saved` folder. Now we load those models and use them to verify the face of a person.\n",
    "\n",
    "The process will be as follow:\n",
    "\n",
    "1. User register their face to the system through sacnning process. After we get sanning images, extract face then extract face embeddings and store them to database.\n",
    "\n",
    "2. When user want to verify their face, we open the camera, capture the image, extract face embeddings and compare with the embeddings in database (with correspoding name use provide when login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T04:48:33.375560Z",
     "start_time": "2024-12-22T04:48:26.880143Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import savez_compressed\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "# For the Facenet model\n",
    "import torch  # Ensure torch is imported here to avoid circular import issues\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each device has a different camera IDs/index, so we need to find the correct camera ID for our device. We try to loop throught a range of camera IDs and ask user to check if the camera is working. Each devices can have many webcams, so we have many corresponding camera IDs, we ask user to choose their preferred camera ID as well.\n",
    "\n",
    "Store these congifuration to the `application_data/settings.json` file. **Next time, when user open the app, these setting will be loaded without asking user again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a application_data folder to store all app related data\n",
    "os.makedirs('application_data', exist_ok=True)\n",
    "\n",
    "# Setting.jpg file path\n",
    "SETTINGS_FILE_PATH = os.path.join('application_data', 'settings.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cameras():\n",
    "   \n",
    "    print(\"Scanning for available cameras\")\n",
    "    detected_cameras = []\n",
    "\n",
    "    # Test cameras 0-9\n",
    "    for cam_id in range(10):\n",
    "        print(f\"\\nTesting camera {cam_id}\")\n",
    "        cap = cv2.VideoCapture(cam_id)\n",
    "\n",
    "\n",
    "    \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            cv2.imshow('Detection camera, if see the camera, press q to quite then type Y to continue', frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Check if camera is detected\n",
    "        response = input(f\"Can you see camera ID {cam_id}? (Y/n): \").lower()\n",
    "        if response == 'y' :\n",
    "            detected_cameras.append(cam_id)\n",
    "\n",
    "\n",
    "    if not detected_cameras:\n",
    "        print(\"No cameras detected!\")\n",
    "        return [], None\n",
    "\n",
    "    # Select preferred camera\n",
    "    preferred = None\n",
    "    if len(detected_cameras) > 1:\n",
    "        while preferred not in detected_cameras:\n",
    "            try:\n",
    "                print(\"\\nAll available cameras:\", detected_cameras)\n",
    "                preferred = int(input(\"Enter the camera ID you want to use (ID 0 is often RGB camera, ID 2 is often IR camera): \"))\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "    else:\n",
    "        preferred = detected_cameras[0]\n",
    "\n",
    "    # Save settings\n",
    "    settings = {\n",
    "        \"camera_list\": detected_cameras,\n",
    "        \"preferred_camera\": preferred\n",
    "    }\n",
    "    \n",
    "    with open(SETTINGS_FILE_PATH, 'w') as f:\n",
    "        json.dump(settings, f)\n",
    "        print(f\"\\nSettings saved to {SETTINGS_FILE_PATH}\")\n",
    "\n",
    "    return detected_cameras, preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When detecting cameras, OpenCV can only open the RGB camera (index 0) and cannot open the IR camera (often index 2) on Windows. On Linux, it can detect all cameras normally.\n",
    "\n",
    "\n",
    "**Next time, just use Linux to develop these things :))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for available cameras\n",
      "\n",
      "Testing camera 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/cta/Project/Biometric_IT4432E/venv/lib/python3.12/site-packages/cv2/qt/plugins\"\n",
      "[ WARN:0@69.684] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@69.786] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 1\n",
      "\n",
      "Testing camera 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@89.379] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@89.479] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@91.768] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@91.769] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@92.721] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video5): can't open camera by index\n",
      "[ERROR:0@92.722] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@93.477] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video6): can't open camera by index\n",
      "[ERROR:0@93.478] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@94.189] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video7): can't open camera by index\n",
      "[ERROR:0@94.190] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@94.891] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video8): can't open camera by index\n",
      "[ERROR:0@94.892] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@95.603] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video9): can't open camera by index\n",
      "[ERROR:0@95.604] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 9\n",
      "\n",
      "All available cameras: [0, 2]\n",
      "\n",
      "Settings saved to application_data/settings.json\n",
      "\n",
      "Detected cameras: [0, 2]\n",
      "Preferred camera: 2\n"
     ]
    }
   ],
   "source": [
    "CAM_ID = 0 # Default camera ID\n",
    "\n",
    "# Check if settings file exists, if no then call the detect camera function\n",
    "if not os.path.exists(SETTINGS_FILE_PATH):\n",
    "    cameras, preferred = detect_cameras()\n",
    "    print(f\"\\nDetected cameras: {cameras}\")\n",
    "    print(f\"Preferred camera: {preferred}\")\n",
    "else:\n",
    "    print(f\"Settings file found at {SETTINGS_FILE_PATH}, if you want to rescan for cameras delete this file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'application_data/settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load the settings file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSETTINGS_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     settings \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m     CAM_ID \u001b[38;5;241m=\u001b[39m settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_camera\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Project/Biometric_IT4432E/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'application_data/settings.json'"
     ]
    }
   ],
   "source": [
    "#Load the settings file\n",
    "with open(SETTINGS_FILE_PATH, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "    CAM_ID = settings['preferred_camera']\n",
    "    \n",
    "print(\"Running app with camera ID:\", CAM_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you meet error like this:\n",
    "\n",
    "```plaintext\n",
    "error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
    "```\n",
    "Consider run:\n",
    "\n",
    "```bash\n",
    "pip uninstall opencv-python-headless\n",
    "pip uninstall opencv-python\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "Then restart the IDE and run the code again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some example of camera IDs:\n",
    "\n",
    "- CAM_ID = 0 for laptop normal webcam\n",
    "- CAM_ID = 2 for laptop IR webcam\n",
    "- CAM_ID = 4 for external webcam\n",
    "\n",
    "\n",
    "***Depend on each devices, these number can be different. Try out all number start from 0 and see which one is the correct one on your device.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enrollment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a application_data folder to store all app related data\n",
    "\n",
    "os.makedirs('application_data', exist_ok=True)\n",
    "\n",
    "# Inside this foilder, create a folder name validation_images to store all the images that are used for validation process\n",
    "valiation_images = os.path.join('application_data', 'validation_images')\n",
    "os.makedirs(valiation_images, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'p' to capture an image, 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the camera and take pictures of the user for sacnning process\n",
    "# Save the images in the validation_images folder, inside a subfolder with the user's name\n",
    "\n",
    "# Function to capture images from webcam\n",
    "def capture_images(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    os.makedirs(user_folder, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(CAM_ID)\n",
    "    print(\"Press 'p' to capture an image, 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Enrollment process, p to capture, q to quit', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            img_name = f\"{uuid.uuid4()}.jpg\"\n",
    "            img_path = os.path.join(user_folder, img_name)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            print(f\"Image saved: {img_path}\")\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ask for user name and capture images\n",
    "user_name = input(\"Enter your name to register to system: \")\n",
    "capture_images(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we process the images to extract the face of a subfolder/person name in the validation_images folder\n",
    "# then store the faces.npz right in that subfolder, using MTCNN to detect faces\n",
    "\n",
    "# Function to detect faces and save to faces.npz\n",
    "# Parameters:\n",
    "# user_name: Name of the user whose images are to be processed\n",
    "def detect_and_save_faces(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    face_folder = os.path.join(user_folder, 'face') # A subfolder with person name already contains\n",
    "    # sacnning images, so make a seperate `face` subfolder isnide that to sotre the faces.npz for better organization\n",
    "    os.makedirs(face_folder, exist_ok=True)\n",
    "    \n",
    "    detector = MTCNN()\n",
    "    faces = []\n",
    "    \n",
    "    for img_file in os.listdir(user_folder): # Loop through all the images in the user folder\n",
    "        if img_file.endswith('.jpg'):\n",
    "            img_path = os.path.join(user_folder, img_file)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_np = np.array(image)\n",
    "            detections = detector.detect_faces(image_np)\n",
    "            \n",
    "            for i, detection in enumerate(detections):\n",
    "                x, y, width, height = detection['box']\n",
    "                face = image_np[y:y+height, x:x+width]\n",
    "                face_image = Image.fromarray(face).resize((160, 160))\n",
    "                face_array = np.array(face_image)\n",
    "                faces.append(face_array)\n",
    "    \n",
    "    faces = np.array(faces)\n",
    "    savez_compressed(os.path.join(face_folder, 'faces.npz'), faces)\n",
    "\n",
    "\n",
    "# Call the function to detect faces and save to faces.npz\n",
    "if user_name:  #username is input from the user at the previous step\n",
    "    detect_and_save_faces(user_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is as follow:\n",
    "\n",
    "```plaintext\n",
    "application_data\n",
    "|\n",
    "|───Validation_images\n",
    "|   |───user1\n",
    "|   |   |───face\n",
    "|   |   |   └───faces.npz\n",
    "|   |   |───image1.jpg\n",
    "|   |   |───image2.jpg\n",
    "|   |   |───...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the above steps, we have the faces.npz file for the user, from that file, we continue\n",
    "# to extract the face embeddings\n",
    "\n",
    "# Load the pre-trained FaceNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "\n",
    "# Define a function to generate embeddings (this function is already defined in\n",
    "# the Preprocessing Notebook, so we can just copy it here)\n",
    "# Parameters:\n",
    "# - image_array: a numpy array representing the image\n",
    "def generate_embedding(image_array, model=facenet_model):\n",
    "    # Convert numpy array to PIL image\n",
    "    image = Image.fromarray(image_array)\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Function to generate embeddings and save to embedding.npz\n",
    "def generate_and_save_embeddings(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    face_folder = os.path.join(user_folder, 'face')\n",
    "    embedding_folder = os.path.join(user_folder, 'embeddings')\n",
    "    if not os.path.exists(embedding_folder):\n",
    "        os.makedirs(embedding_folder)\n",
    "    \n",
    "    data = np.load(os.path.join(face_folder, 'faces.npz'))\n",
    "    faces = data['arr_0']\n",
    "    embeddings = []\n",
    "    \n",
    "    for face in faces:\n",
    "        embedding = generate_embedding(face)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    savez_compressed(os.path.join(embedding_folder, 'embeddings.npz'), embeddings)\n",
    "\n",
    "# Call the function to generate embeddings and save to embeddings.npz\n",
    "if user_name: # if user_name not null\n",
    "    generate_and_save_embeddings(user_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is as follow:\n",
    "\n",
    "```plaintext\n",
    "application_data\n",
    "|\n",
    "|───Validation_images\n",
    "|   |───user1\n",
    "|   |   |───face\n",
    "|   |   |   └───faces.npz\n",
    "|   |   |\n",
    "|   |   |───embeddings\n",
    "|   |   |   └───embeddings.npz\n",
    "|   |   |\n",
    "|   |   |───image1.jpg\n",
    "|   |   |───image2.jpg\n",
    "|   |   |───...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal is just the `embeddings.npz` file, other face.npz, images are just for vizuale the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Verification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected in the input image or the user is not found in the system\n"
     ]
    }
   ],
   "source": [
    "# When use wnat to login, capture a image from the webcam when user press 'v' and \n",
    "# then compare that input image with the all the embeddings of the user to check if the user is the same person\n",
    "\n",
    "# Function to capture a single image directly from webcam, return the frame object\n",
    "def verify_user(username, detector, facenet_model):\n",
    "\n",
    "\n",
    "    # Frist, check if user existed in the system\n",
    "    if not os.path.exists(os.path.join('application_data/validation_images', username)):\n",
    "        print(\"User not found\")\n",
    "        # Then return an empty list\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(CAM_ID)\n",
    "    # Declare a variable frame to store the captured image\n",
    "    frame = None\n",
    "\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Verify user. Press v to capture an image', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('v'):\n",
    "            embedding_of_input = extract_face_and_generate_embedding(frame, detector, facenet_model)\n",
    "            result = compare_embeddings(username,embedding_of_input, svm_model, scaler)\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return result\n",
    "\n",
    "   \n",
    "\n",
    "# After capture image, extract the face from the image and generate the embeddings\n",
    "def extract_face_and_generate_embedding(frame, detector, facenet_model):\n",
    "    \n",
    "    faces = []\n",
    "    \n",
    "    image = Image.fromarray(frame).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    detections = detector.detect_faces(image)\n",
    "    \n",
    "    for i, detection in enumerate(detections):\n",
    "        x, y, width, height = detection['box']\n",
    "        face = image[y:y+height, x:x+width]\n",
    "        face_image = Image.fromarray(face).resize((160, 160))\n",
    "        face_array = np.array(face_image)\n",
    "        faces.append(face_array)\n",
    "    \n",
    "    faces = np.array(faces)\n",
    "    embedding = generate_embedding(faces[0], facenet_model) # Since we are capturing a single image, we only have one face\n",
    "    return embedding\n",
    "\n",
    "def generate_embedding(image_array, model=facenet_model):\n",
    "    # Convert numpy array to PIL image\n",
    "    image = Image.fromarray(image_array)\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# Function to compare the input image with all embeddings of the user using the model we pass in\n",
    "\n",
    "def compare_embeddings(username, embedding_of_input, svm_model, scaler):\n",
    "    user_folder = os.path.join('application_data/validation_images', username)\n",
    "    embedding_folder = os.path.join(user_folder, 'embeddings')\n",
    "    \n",
    "\n",
    "    data = np.load(os.path.join(embedding_folder, 'embeddings.npz'))\n",
    "    embeddings = data['arr_0']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for validation_embedding in embeddings:\n",
    "        \n",
    "        # Flatten the embeddings\n",
    "        input_embedding_flat = embedding_of_input.flatten()\n",
    "        validation_embedding_flat = validation_embedding.flatten()\n",
    "        \n",
    "        pair = np.concatenate((input_embedding_flat, validation_embedding_flat))\n",
    "        pair_scaled = scaler.transform([pair])\n",
    "        \n",
    "        prediction = svm_model.predict(pair_scaled)\n",
    "        probabilities = svm_model.predict_proba(pair_scaled)\n",
    "        print(\"Compare with validation image, rresult is:\", prediction[0], 'with confidence: ', probabilities)\n",
    "        results.append(prediction[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Main function\n",
    "\n",
    "# Declare all model to use\n",
    "detector = MTCNN()\n",
    "svm_model = None\n",
    "scaler = None\n",
    "# Load the pre-trained FaceNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "\n",
    "\n",
    "with open('./model_saved/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "with open('./model_saved/svm_model.pkl', 'rb') as f:\n",
    "        svm_model = pickle.load(f)\n",
    "\n",
    "# Prompt who are trying to login\n",
    "username = input(\"Who are you ?\")\n",
    "result = []\n",
    "if username: # prevent the case user enter nothing\n",
    "    result = verify_user(username, detector, facenet_model)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Now, based on the result, we can decide if the user is the same person or not with a threshold. \n",
    "# If the proportion of output 1 / total output is greater than a threshold, we can say the user is the same person\n",
    "# Define a threshold\n",
    "threshold = 0.8\n",
    "\n",
    "# Calculate the proportion of positive identifications\n",
    "positive_identifications = sum(result)\n",
    "total_identifications = len(result)\n",
    "\n",
    "if total_identifications == 0:\n",
    "    print(\"No face detected in the input image or the user is not found in the system\")\n",
    "else: \n",
    "    proportion = positive_identifications / total_identifications\n",
    "\n",
    "    # Determine if the user is the same person\n",
    "    if proportion > threshold:\n",
    "        print(\"User verified successfully.\")\n",
    "    else:\n",
    "        print(\"User verification failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for face verification app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies and CAM ID setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed training some model and store those models to `model_saved` folder. Now we load those models and use them to verify the face of a person.\n",
    "\n",
    "The process will be as follow:\n",
    "\n",
    "1. User register their face to the system through sacnning process. After we get sanning images, extract face then extract face embeddings and store them to database.\n",
    "\n",
    "2. When user want to verify their face, we open the camera, capture the image, extract face embeddings and compare with the embeddings in database (with correspoding name use provide when login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 17:06:07.499876: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-21 17:06:07.513132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734775567.525377   52405 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734775567.528401   52405 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-21 17:06:07.540896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/cta/Project/Biometric_IT4432E/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import savez_compressed\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "# For the Facenet model\n",
    "import torch  # Ensure torch is imported here to avoid circular import issues\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each device has a different camera IDs/index, so we need to find the correct camera ID for our device. We try to loop throught a range of camera IDs and ask user to check if the camera is working. Each devices can have many webcams, so we have many corresponding camera IDs, we ask user to choose their prefered camera ID as well.\n",
    "\n",
    "Store these congifuration to the `application_data/setting.json` file. **Next time, when user open the app, these setting will be loaded without asking user again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a application_data folder to store all app related data\n",
    "os.makedirs('application_data', exist_ok=True)\n",
    "\n",
    "# Setting.jpg file path\n",
    "SETTINGS_FILE_PATH = os.path.join('application_data', 'settings.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cameras():\n",
    "   \n",
    "    print(\"Scanning for available cameras\")\n",
    "    detected_cameras = []\n",
    "\n",
    "    # Test cameras 0-9\n",
    "    for cam_id in range(10):\n",
    "        print(f\"\\nTesting camera {cam_id}\")\n",
    "        cap = cv2.VideoCapture(cam_id)\n",
    "        if cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                cv2.imshow(f'Camera {cam_id}', frame)\n",
    "                cv2.waitKey(1000)\n",
    "                \n",
    "                response = input(f\"Can you see camera ID {cam_id}? (Y/n): \").lower()\n",
    "                if response == 'y' or response == 'Y':\n",
    "                    detected_cameras.append(cam_id)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                cv2.destroyAllWindows()\n",
    "            cap.release()\n",
    "\n",
    "    if not detected_cameras:\n",
    "        print(\"No cameras detected!\")\n",
    "        return [], None\n",
    "\n",
    "    # Select preferred camera\n",
    "    preferred = None\n",
    "    if len(detected_cameras) > 1:\n",
    "        while preferred not in detected_cameras:\n",
    "            try:\n",
    "                print(\"\\nAll available cameras:\", detected_cameras)\n",
    "                preferred = int(input(\"Enter the camera ID you want to use (ID 0 is often RGB camera, ID 2 is often IR camera): \"))\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "    else:\n",
    "        preferred = detected_cameras[0]\n",
    "\n",
    "    # Save settings\n",
    "    settings = {\n",
    "        \"camera_list\": detected_cameras,\n",
    "        \"preferred_camera\": preferred\n",
    "    }\n",
    "    \n",
    "    with open(SETTINGS_FILE_PATH, 'w') as f:\n",
    "        json.dump(settings, f)\n",
    "        print(f\"\\nSettings saved to {SETTINGS_FILE_PATH}\")\n",
    "\n",
    "    return detected_cameras, preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for available cameras\n",
      "\n",
      "Testing camera 0\n",
      "\n",
      "Testing camera 1\n",
      "\n",
      "Testing camera 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@664.662] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@664.663] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.739] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@667.741] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.741] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@667.741] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.742] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video5): can't open camera by index\n",
      "[ERROR:0@667.742] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.742] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video6): can't open camera by index\n",
      "[ERROR:0@667.742] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.742] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video7): can't open camera by index\n",
      "[ERROR:0@667.743] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.743] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video8): can't open camera by index\n",
      "[ERROR:0@667.743] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@667.743] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video9): can't open camera by index\n",
      "[ERROR:0@667.744] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing camera 3\n",
      "\n",
      "Testing camera 4\n",
      "\n",
      "Testing camera 5\n",
      "\n",
      "Testing camera 6\n",
      "\n",
      "Testing camera 7\n",
      "\n",
      "Testing camera 8\n",
      "\n",
      "Testing camera 9\n",
      "\n",
      "All available cameras: [0, 2]\n",
      "\n",
      "Settings saved to application_data/settings.json\n",
      "\n",
      "Detected cameras: [0, 2]\n",
      "Preferred camera: 2\n"
     ]
    }
   ],
   "source": [
    "CAM_ID = 0 # Default camera ID\n",
    "\n",
    "# Check if settings file exists, if no then call the detect camera function\n",
    "if not os.path.exists(SETTINGS_FILE_PATH):\n",
    "    cameras, preferred = detect_cameras()\n",
    "    print(f\"\\nDetected cameras: {cameras}\")\n",
    "    print(f\"Preferred camera: {preferred}\")\n",
    "else:\n",
    "    print(f\"Settings file found at {SETTINGS_FILE_PATH}, if you want to rescan for cameras delete this file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnin app with camera ID: 2\n"
     ]
    }
   ],
   "source": [
    "#Load the settings file\n",
    "with open(SETTINGS_FILE_PATH, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "    CAM_ID = settings['preferred_camera']\n",
    "    \n",
    "print(\"Runnin app with camera ID:\", CAM_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some example of camera IDs:\n",
    "\n",
    "- CAM_ID = 0 for laptop normal webcam\n",
    "- CAM_ID = 2 for laptop IR webcam\n",
    "- CAM_ID = 4 for external webcam\n",
    "\n",
    "\n",
    "***Depend on each devices, these number can be different. Try out all number start from 0 and see which one is the correct one on your device.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enrollment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a application_data folder to store all app related data\n",
    "\n",
    "os.makedirs('application_data', exist_ok=True)\n",
    "\n",
    "# Inside this foilder, create a folder name validation_images to store all the images that are used for validation process\n",
    "valiation_images = os.path.join('application_data', 'validation_images')\n",
    "os.makedirs(valiation_images, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'p' to capture an image, 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/cta/Project/Biometric_IT4432E/venv/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved: application_data/validation_images/cta2/af5ffc19-17d2-4e11-8a30-ece5ca19ee86.jpg\n",
      "Image saved: application_data/validation_images/cta2/1e7acb7e-a856-4d45-b601-48bdec9f080d.jpg\n",
      "Image saved: application_data/validation_images/cta2/578eb15a-1a77-4eea-986c-6e9f1c88da40.jpg\n",
      "Image saved: application_data/validation_images/cta2/8b617820-efea-4927-a991-362028a58a67.jpg\n",
      "Image saved: application_data/validation_images/cta2/a4593ea0-df1e-4d9c-8de4-db595379c415.jpg\n",
      "Image saved: application_data/validation_images/cta2/03127a22-09ea-4561-8c25-34fe37292181.jpg\n",
      "Image saved: application_data/validation_images/cta2/0e9ae752-d48d-4b26-9ade-c2aa090f17a3.jpg\n",
      "Image saved: application_data/validation_images/cta2/bffd20b6-572c-4a7c-8128-042832a6b36d.jpg\n",
      "Image saved: application_data/validation_images/cta2/a48b99c9-55c9-4b39-8882-6ab97d944712.jpg\n",
      "Image saved: application_data/validation_images/cta2/5a60786e-b685-4ac8-aff0-70a6033952c7.jpg\n",
      "Image saved: application_data/validation_images/cta2/ef3c6601-707d-4345-b6f2-455ae825738f.jpg\n",
      "Image saved: application_data/validation_images/cta2/1e261d73-31b1-45b1-891a-e6a4342401eb.jpg\n",
      "Image saved: application_data/validation_images/cta2/453585a9-8f37-44cf-9661-cce4961c990f.jpg\n"
     ]
    }
   ],
   "source": [
    "# Connect to the camera and take pictures of the user for sacnning process\n",
    "# Save the images in the validation_images folder, inside a subfolder with the user's name\n",
    "\n",
    "# Function to capture images from webcam\n",
    "def capture_images(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    os.makedirs(user_folder, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(CAM_ID)\n",
    "    print(\"Press 'p' to capture an image, 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Enrollment process, p to capture, q to quit', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            img_name = f\"{uuid.uuid4()}.jpg\"\n",
    "            img_path = os.path.join(user_folder, img_name)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            print(f\"Image saved: {img_path}\")\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ask for user name and capture images\n",
    "user_name = input(\"Enter your name to register to system: \")\n",
    "capture_images(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we process the images to extract the face of a subfolder/person name in the validation_images folder\n",
    "# then store the faces.npz right in that subfolder, using MTCNN to detect faces\n",
    "\n",
    "# Function to detect faces and save to faces.npz\n",
    "# Parameters:\n",
    "# user_name: Name of the user whose images are to be processed\n",
    "def detect_and_save_faces(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    face_folder = os.path.join(user_folder, 'face') # A subfolder with person name already contains\n",
    "    # sacnning images, so make a seperate `face` subfolder isnide that to sotre the faces.npz for better organization\n",
    "    os.makedirs(face_folder, exist_ok=True)\n",
    "    \n",
    "    detector = MTCNN()\n",
    "    faces = []\n",
    "    \n",
    "    for img_file in os.listdir(user_folder): # Loop through all the images in the user folder\n",
    "        if img_file.endswith('.jpg'):\n",
    "            img_path = os.path.join(user_folder, img_file)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_np = np.array(image)\n",
    "            detections = detector.detect_faces(image_np)\n",
    "            \n",
    "            for i, detection in enumerate(detections):\n",
    "                x, y, width, height = detection['box']\n",
    "                face = image_np[y:y+height, x:x+width]\n",
    "                face_image = Image.fromarray(face).resize((160, 160))\n",
    "                face_array = np.array(face_image)\n",
    "                faces.append(face_array)\n",
    "    \n",
    "    faces = np.array(faces)\n",
    "    savez_compressed(os.path.join(face_folder, 'faces.npz'), faces)\n",
    "\n",
    "\n",
    "# Call the function to detect faces and save to faces.npz\n",
    "if user_name:  #username is input from the user at the previous step\n",
    "    detect_and_save_faces(user_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is as follow:\n",
    "\n",
    "```plaintext\n",
    "application_data\n",
    "|\n",
    "|───Validation_images\n",
    "|   |───user1\n",
    "|   |   |───face\n",
    "|   |   |   └───faces.npz\n",
    "|   |   |───image1.jpg\n",
    "|   |   |───image2.jpg\n",
    "|   |   |───...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the above steps, we have the faces.npz file for the user, from that file, we continue\n",
    "# to extract the face embeddings\n",
    "\n",
    "# Load the pre-trained FaceNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "\n",
    "# Define a function to generate embeddings (this function is already defined in\n",
    "# the Preprocessing Notebook, so we can just copy it here)\n",
    "# Parameters:\n",
    "# - image_array: a numpy array representing the image\n",
    "def generate_embedding(image_array, model=facenet_model):\n",
    "    # Convert numpy array to PIL image\n",
    "    image = Image.fromarray(image_array)\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Function to generate embeddings and save to embedding.npz\n",
    "def generate_and_save_embeddings(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    face_folder = os.path.join(user_folder, 'face')\n",
    "    embedding_folder = os.path.join(user_folder, 'embeddings')\n",
    "    if not os.path.exists(embedding_folder):\n",
    "        os.makedirs(embedding_folder)\n",
    "    \n",
    "    data = np.load(os.path.join(face_folder, 'faces.npz'))\n",
    "    faces = data['arr_0']\n",
    "    embeddings = []\n",
    "    \n",
    "    for face in faces:\n",
    "        embedding = generate_embedding(face)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    savez_compressed(os.path.join(embedding_folder, 'embeddings.npz'), embeddings)\n",
    "\n",
    "# Call the function to generate embeddings and save to embeddings.npz\n",
    "if user_name: # if user_name not null\n",
    "    generate_and_save_embeddings(user_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is as follow:\n",
    "\n",
    "```plaintext\n",
    "application_data\n",
    "|\n",
    "|───Validation_images\n",
    "|   |───user1\n",
    "|   |   |───face\n",
    "|   |   |   └───faces.npz\n",
    "|   |   |\n",
    "|   |   |───embeddings\n",
    "|   |   |   └───embeddings.npz\n",
    "|   |   |\n",
    "|   |   |───image1.jpg\n",
    "|   |   |───image2.jpg\n",
    "|   |   |───...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal is just the `embeddings.npz` file, other face.npz, images are just for vizuale the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Verification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare with validation image, rresult is: 1 with confidence:  [[0.27835947 0.72164053]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.06641595 0.93358405]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.15238859 0.84761141]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.08142101 0.91857899]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.06905452 0.93094548]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.04203467 0.95796533]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.04513154 0.95486846]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.18082076 0.81917924]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.12268198 0.87731802]]\n",
      "Compare with validation image, rresult is: 0 with confidence:  [[0.50892698 0.49107302]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.16536213 0.83463787]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.09267407 0.90732593]]\n",
      "Compare with validation image, rresult is: 0 with confidence:  [[0.44845991 0.55154009]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n",
      "User verified successfully.\n"
     ]
    }
   ],
   "source": [
    "# When use wnat to login, capture a image from the webcam when user press 'v' and \n",
    "# then compare that input image with the all the embeddings of the user to check if the user is the same person\n",
    "\n",
    "# Function to capture a single image directly from webcam, return the frame object\n",
    "def verify_user(username, detector, facenet_model):\n",
    "\n",
    "\n",
    "    # Frist, check if user existed in the system\n",
    "    if not os.path.exists(os.path.join('application_data/validation_images', username)):\n",
    "        print(\"User not found\")\n",
    "        # Then return an empty list\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(CAM_ID)\n",
    "    # Declare a variable frame to store the captured image\n",
    "    frame = None\n",
    "\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Verify user. Press v to capture an image', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('v'):\n",
    "            embedding_of_input = extract_face_and_generate_embedding(frame, detector, facenet_model)\n",
    "            result = compare_embeddings(username,embedding_of_input, svm_model, scaler)\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return result\n",
    "\n",
    "   \n",
    "\n",
    "# After capture image, extract the face from the image and generate the embeddings\n",
    "def extract_face_and_generate_embedding(frame, detector, facenet_model):\n",
    "    \n",
    "    faces = []\n",
    "    \n",
    "    image = Image.fromarray(frame).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    detections = detector.detect_faces(image)\n",
    "    \n",
    "    for i, detection in enumerate(detections):\n",
    "        x, y, width, height = detection['box']\n",
    "        face = image[y:y+height, x:x+width]\n",
    "        face_image = Image.fromarray(face).resize((160, 160))\n",
    "        face_array = np.array(face_image)\n",
    "        faces.append(face_array)\n",
    "    \n",
    "    faces = np.array(faces)\n",
    "    embedding = generate_embedding(faces[0], facenet_model) # Since we are capturing a single image, we only have one face\n",
    "    return embedding\n",
    "\n",
    "def generate_embedding(image_array, model=facenet_model):\n",
    "    # Convert numpy array to PIL image\n",
    "    image = Image.fromarray(image_array)\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# Function to compare the input image with all embeddings of the user using the model we pass in\n",
    "\n",
    "def compare_embeddings(username, embedding_of_input, svm_model, scaler):\n",
    "    user_folder = os.path.join('application_data/validation_images', username)\n",
    "    embedding_folder = os.path.join(user_folder, 'embeddings')\n",
    "    \n",
    "\n",
    "    data = np.load(os.path.join(embedding_folder, 'embeddings.npz'))\n",
    "    embeddings = data['arr_0']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for validation_embedding in embeddings:\n",
    "        \n",
    "        # Flatten the embeddings\n",
    "        input_embedding_flat = embedding_of_input.flatten()\n",
    "        validation_embedding_flat = validation_embedding.flatten()\n",
    "        \n",
    "        pair = np.concatenate((input_embedding_flat, validation_embedding_flat))\n",
    "        pair_scaled = scaler.transform([pair])\n",
    "        \n",
    "        prediction = svm_model.predict(pair_scaled)\n",
    "        probabilities = svm_model.predict_proba(pair_scaled)\n",
    "        print(\"Compare with validation image, rresult is:\", prediction[0], 'with confidence: ', probabilities)\n",
    "        results.append(prediction[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Main function\n",
    "\n",
    "# Declare all model to use\n",
    "detector = MTCNN()\n",
    "svm_model = None\n",
    "scaler = None\n",
    "# Load the pre-trained FaceNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "\n",
    "\n",
    "with open('./model_saved/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "with open('./model_saved/svm_model.pkl', 'rb') as f:\n",
    "        svm_model = pickle.load(f)\n",
    "\n",
    "# Prompt who are trying to login\n",
    "username = input(\"Who are you ?\")\n",
    "result = []\n",
    "if username: # prevent the case user enter nothing\n",
    "    result = verify_user(username, detector, facenet_model)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Now, based on the result, we can decide if the user is the same person or not with a threshold. \n",
    "# If the proportion of output 1 / total output is greater than a threshold, we can say the user is the same person\n",
    "# Define a threshold\n",
    "threshold = 0.8\n",
    "\n",
    "# Calculate the proportion of positive identifications\n",
    "positive_identifications = sum(result)\n",
    "total_identifications = len(result)\n",
    "\n",
    "if total_identifications == 0:\n",
    "    print(\"No face detected in the input image or the user is not found in the system\")\n",
    "else: \n",
    "    proportion = positive_identifications / total_identifications\n",
    "\n",
    "    # Determine if the user is the same person\n",
    "    if proportion > threshold:\n",
    "        print(\"User verified successfully.\")\n",
    "    else:\n",
    "        print(\"User verification failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

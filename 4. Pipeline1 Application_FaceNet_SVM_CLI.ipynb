{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for face verification app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed training some model and store those models to `model_saved` folder. Now we load those models and use them to verify the face of a person.\n",
    "\n",
    "The process will be as follow:\n",
    "\n",
    "1. User register their face to the system through sacnning process. After we get sanning images, extract face then extract face embeddings and store them to database.\n",
    "\n",
    "2. When user want to verify their face, we open the camera, capture the image, extract face embeddings and compare with the embeddings in database (with correspoding name use provide when login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import savez_compressed\n",
    "import pickle\n",
    "\n",
    "# For the Facenet model\n",
    "import torch  # Ensure torch is imported here to avoid circular import issues\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we defined the camera that the system will connect to:\n",
    "\n",
    "- CAM_ID = 1 for laptop normal webcam\n",
    "- CAM_ID = 3 for laptop IR webcam\n",
    "- CAM_ID = 5 for external webcam\n",
    "\n",
    "\n",
    "***Depend on each devices, these number can be different. Try out all number start from 0 and see which one is the correct one on your device.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAM_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enrollment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a application_data folder to store all app related data\n",
    "\n",
    "os.makedirs('application_data', exist_ok=True)\n",
    "\n",
    "# Inside this foilder, create a folder name validation_images to store all the images that are used for validation process\n",
    "valiation_images = os.path.join('application_data', 'validation_images')\n",
    "os.makedirs(valiation_images, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'p' to capture an image, 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/cta/Project/Biometric_IT4432E/venv/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved: application_data/validation_images/cta2/af5ffc19-17d2-4e11-8a30-ece5ca19ee86.jpg\n",
      "Image saved: application_data/validation_images/cta2/1e7acb7e-a856-4d45-b601-48bdec9f080d.jpg\n",
      "Image saved: application_data/validation_images/cta2/578eb15a-1a77-4eea-986c-6e9f1c88da40.jpg\n",
      "Image saved: application_data/validation_images/cta2/8b617820-efea-4927-a991-362028a58a67.jpg\n",
      "Image saved: application_data/validation_images/cta2/a4593ea0-df1e-4d9c-8de4-db595379c415.jpg\n",
      "Image saved: application_data/validation_images/cta2/03127a22-09ea-4561-8c25-34fe37292181.jpg\n",
      "Image saved: application_data/validation_images/cta2/0e9ae752-d48d-4b26-9ade-c2aa090f17a3.jpg\n",
      "Image saved: application_data/validation_images/cta2/bffd20b6-572c-4a7c-8128-042832a6b36d.jpg\n",
      "Image saved: application_data/validation_images/cta2/a48b99c9-55c9-4b39-8882-6ab97d944712.jpg\n",
      "Image saved: application_data/validation_images/cta2/5a60786e-b685-4ac8-aff0-70a6033952c7.jpg\n",
      "Image saved: application_data/validation_images/cta2/ef3c6601-707d-4345-b6f2-455ae825738f.jpg\n",
      "Image saved: application_data/validation_images/cta2/1e261d73-31b1-45b1-891a-e6a4342401eb.jpg\n",
      "Image saved: application_data/validation_images/cta2/453585a9-8f37-44cf-9661-cce4961c990f.jpg\n"
     ]
    }
   ],
   "source": [
    "# Connect to the camera and take pictures of the user for sacnning process\n",
    "# Save the images in the validation_images folder, inside a subfolder with the user's name\n",
    "\n",
    "# Function to capture images from webcam\n",
    "def capture_images(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    os.makedirs(user_folder, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(CAM_ID)\n",
    "    print(\"Press 'p' to capture an image, 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Enrollment process, p to capture, q to quit', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            img_name = f\"{uuid.uuid4()}.jpg\"\n",
    "            img_path = os.path.join(user_folder, img_name)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            print(f\"Image saved: {img_path}\")\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ask for user name and capture images\n",
    "user_name = input(\"Enter your name to register to system: \")\n",
    "capture_images(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we process the images to extract the face of a subfolder/person name in the validation_images folder\n",
    "# then store the faces.npz right in that subfolder, using MTCNN to detect faces\n",
    "\n",
    "# Function to detect faces and save to faces.npz\n",
    "# Parameters:\n",
    "# user_name: Name of the user whose images are to be processed\n",
    "def detect_and_save_faces(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    face_folder = os.path.join(user_folder, 'face') # A subfolder with person name already contains\n",
    "    # sacnning images, so make a seperate `face` subfolder isnide that to sotre the faces.npz for better organization\n",
    "    os.makedirs(face_folder, exist_ok=True)\n",
    "    \n",
    "    detector = MTCNN()\n",
    "    faces = []\n",
    "    \n",
    "    for img_file in os.listdir(user_folder): # Loop through all the images in the user folder\n",
    "        if img_file.endswith('.jpg'):\n",
    "            img_path = os.path.join(user_folder, img_file)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_np = np.array(image)\n",
    "            detections = detector.detect_faces(image_np)\n",
    "            \n",
    "            for i, detection in enumerate(detections):\n",
    "                x, y, width, height = detection['box']\n",
    "                face = image_np[y:y+height, x:x+width]\n",
    "                face_image = Image.fromarray(face).resize((160, 160))\n",
    "                face_array = np.array(face_image)\n",
    "                faces.append(face_array)\n",
    "    \n",
    "    faces = np.array(faces)\n",
    "    savez_compressed(os.path.join(face_folder, 'faces.npz'), faces)\n",
    "\n",
    "\n",
    "# Call the function to detect faces and save to faces.npz\n",
    "if user_name:  #username is input from the user at the previous step\n",
    "    detect_and_save_faces(user_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is as follow:\n",
    "\n",
    "```plaintext\n",
    "application_data\n",
    "|\n",
    "|───Validation_images\n",
    "|   |───user1\n",
    "|   |   |───face\n",
    "|   |   |   └───faces.npz\n",
    "|   |   |───image1.jpg\n",
    "|   |   |───image2.jpg\n",
    "|   |   |───...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the above steps, we have the faces.npz file for the user, from that file, we continue\n",
    "# to extract the face embeddings\n",
    "\n",
    "# Load the pre-trained FaceNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "\n",
    "# Define a function to generate embeddings (this function is already defined in\n",
    "# the Preprocessing Notebook, so we can just copy it here)\n",
    "# Parameters:\n",
    "# - image_array: a numpy array representing the image\n",
    "def generate_embedding(image_array, model=facenet_model):\n",
    "    # Convert numpy array to PIL image\n",
    "    image = Image.fromarray(image_array)\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Function to generate embeddings and save to embedding.npz\n",
    "def generate_and_save_embeddings(user_name, store_location='application_data/validation_images'):\n",
    "    user_folder = os.path.join(store_location, user_name)\n",
    "    face_folder = os.path.join(user_folder, 'face')\n",
    "    embedding_folder = os.path.join(user_folder, 'embeddings')\n",
    "    if not os.path.exists(embedding_folder):\n",
    "        os.makedirs(embedding_folder)\n",
    "    \n",
    "    data = np.load(os.path.join(face_folder, 'faces.npz'))\n",
    "    faces = data['arr_0']\n",
    "    embeddings = []\n",
    "    \n",
    "    for face in faces:\n",
    "        embedding = generate_embedding(face)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    savez_compressed(os.path.join(embedding_folder, 'embeddings.npz'), embeddings)\n",
    "\n",
    "# Call the function to generate embeddings and save to embeddings.npz\n",
    "if user_name: # if user_name not null\n",
    "    generate_and_save_embeddings(user_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is as follow:\n",
    "\n",
    "```plaintext\n",
    "application_data\n",
    "|\n",
    "|───Validation_images\n",
    "|   |───user1\n",
    "|   |   |───face\n",
    "|   |   |   └───faces.npz\n",
    "|   |   |\n",
    "|   |   |───embeddings\n",
    "|   |   |   └───embeddings.npz\n",
    "|   |   |\n",
    "|   |   |───image1.jpg\n",
    "|   |   |───image2.jpg\n",
    "|   |   |───...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal is just the `embeddings.npz` file, other face.npz, images are just for vizuale the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Verification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare with validation image, rresult is: 1 with confidence:  [[0.27835947 0.72164053]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.06641595 0.93358405]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.15238859 0.84761141]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.08142101 0.91857899]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.06905452 0.93094548]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.04203467 0.95796533]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.04513154 0.95486846]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.18082076 0.81917924]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.12268198 0.87731802]]\n",
      "Compare with validation image, rresult is: 0 with confidence:  [[0.50892698 0.49107302]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.16536213 0.83463787]]\n",
      "Compare with validation image, rresult is: 1 with confidence:  [[0.09267407 0.90732593]]\n",
      "Compare with validation image, rresult is: 0 with confidence:  [[0.44845991 0.55154009]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n",
      "User verified successfully.\n"
     ]
    }
   ],
   "source": [
    "# When use wnat to login, capture a image from the webcam when user press 'v' and \n",
    "# then compare that input image with the all the embeddings of the user to check if the user is the same person\n",
    "\n",
    "# Function to capture a single image directly from webcam, return the frame object\n",
    "def verify_user(username, detector, facenet_model):\n",
    "\n",
    "\n",
    "    # Frist, check if user existed in the system\n",
    "    if not os.path.exists(os.path.join('application_data/validation_images', username)):\n",
    "        print(\"User not found\")\n",
    "        # Then return an empty list\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(CAM_ID)\n",
    "    # Declare a variable frame to store the captured image\n",
    "    frame = None\n",
    "\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Verify user. Press v to capture an image', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('v'):\n",
    "            embedding_of_input = extract_face_and_generate_embedding(frame, detector, facenet_model)\n",
    "            result = compare_embeddings(username,embedding_of_input, svm_model, scaler)\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return result\n",
    "\n",
    "   \n",
    "\n",
    "# After capture image, extract the face from the image and generate the embeddings\n",
    "def extract_face_and_generate_embedding(frame, detector, facenet_model):\n",
    "    \n",
    "    faces = []\n",
    "    \n",
    "    image = Image.fromarray(frame).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    detections = detector.detect_faces(image)\n",
    "    \n",
    "    for i, detection in enumerate(detections):\n",
    "        x, y, width, height = detection['box']\n",
    "        face = image[y:y+height, x:x+width]\n",
    "        face_image = Image.fromarray(face).resize((160, 160))\n",
    "        face_array = np.array(face_image)\n",
    "        faces.append(face_array)\n",
    "    \n",
    "    faces = np.array(faces)\n",
    "    embedding = generate_embedding(faces[0], facenet_model) # Since we are capturing a single image, we only have one face\n",
    "    return embedding\n",
    "\n",
    "def generate_embedding(image_array, model=facenet_model):\n",
    "    # Convert numpy array to PIL image\n",
    "    image = Image.fromarray(image_array)\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# Function to compare the input image with all embeddings of the user using the model we pass in\n",
    "\n",
    "def compare_embeddings(username, embedding_of_input, svm_model, scaler):\n",
    "    user_folder = os.path.join('application_data/validation_images', username)\n",
    "    embedding_folder = os.path.join(user_folder, 'embeddings')\n",
    "    \n",
    "\n",
    "    data = np.load(os.path.join(embedding_folder, 'embeddings.npz'))\n",
    "    embeddings = data['arr_0']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for validation_embedding in embeddings:\n",
    "        \n",
    "        # Flatten the embeddings\n",
    "        input_embedding_flat = embedding_of_input.flatten()\n",
    "        validation_embedding_flat = validation_embedding.flatten()\n",
    "        \n",
    "        pair = np.concatenate((input_embedding_flat, validation_embedding_flat))\n",
    "        pair_scaled = scaler.transform([pair])\n",
    "        \n",
    "        prediction = svm_model.predict(pair_scaled)\n",
    "        probabilities = svm_model.predict_proba(pair_scaled)\n",
    "        print(\"Compare with validation image, rresult is:\", prediction[0], 'with confidence: ', probabilities)\n",
    "        results.append(prediction[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Main function\n",
    "\n",
    "# Declare all model to use\n",
    "detector = MTCNN()\n",
    "svm_model = None\n",
    "scaler = None\n",
    "# Load the pre-trained FaceNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "\n",
    "\n",
    "with open('./model_saved/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "with open('./model_saved/svm_model.pkl', 'rb') as f:\n",
    "        svm_model = pickle.load(f)\n",
    "\n",
    "# Prompt who are trying to login\n",
    "username = input(\"Who are you ?\")\n",
    "result = []\n",
    "if username: # prevent the case user enter nothing\n",
    "    result = verify_user(username, detector, facenet_model)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Now, based on the result, we can decide if the user is the same person or not with a threshold. \n",
    "# If the proportion of output 1 / total output is greater than a threshold, we can say the user is the same person\n",
    "# Define a threshold\n",
    "threshold = 0.8\n",
    "\n",
    "# Calculate the proportion of positive identifications\n",
    "positive_identifications = sum(result)\n",
    "total_identifications = len(result)\n",
    "\n",
    "if total_identifications == 0:\n",
    "    print(\"No face detected in the input image or the user is not found in the system\")\n",
    "else: \n",
    "    proportion = positive_identifications / total_identifications\n",
    "\n",
    "    # Determine if the user is the same person\n",
    "    if proportion > threshold:\n",
    "        print(\"User verified successfully.\")\n",
    "    else:\n",
    "        print(\"User verification failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

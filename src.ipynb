{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 1\n",
    "\n",
    "## Objective\n",
    "\n",
    "General setup:\n",
    "\n",
    "- Install dependencies\n",
    "- Import TensorFlow Layers\n",
    "- Config GPU\n",
    "- Create Data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install and import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a virtual environment for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%python3 -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "#%source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import some necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV\n",
    "import os  # For file operations\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # For plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow dependencies\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D,  Dense, MaxPool2D, Flatten, Input\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv2D for Convolutional Neural Networks\n",
    "- Dense for Fully Connected Neural Networks\n",
    "- MaxPooling2D to pull our layers together and effectively reduce the number of parameters and computations in the network\n",
    "- Flatten take output from previous layer and flatten it to a vector to be fed into the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Setup GPU Limit for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid Memory Error when TensorFlow use two much VRAM\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus # Show all GPUs available in machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create Data Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform facial verification, we actually want to compare two images and determine if they are of the same person. This is a one-shot learning problem. We will learn a function that maps our input data (images) to a feature space such that the distance between the features can be used to determine if the images are of the same person.\n",
    "\n",
    "We will create three data folders:\n",
    "\n",
    "- anchors\n",
    "- positive\n",
    "- negative\n",
    "\n",
    "The anchor image is the image of the person we want to verify.(Input)\n",
    "The positive or negative are verification images. The positive is an image of the same person as the anchor image but taken under different conditions. The negative image is an image of a different person.\n",
    "\n",
    "When we pass in a anchor image, if:\n",
    "\n",
    "- it matches the positive image, the output should be 1 (same)\n",
    "- it matches the negative image, the output should be 0 (different)\n",
    "\n",
    "We want our model to distinguish between outselves and others person. The process will be as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix spelling later :)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AnchorAndPositive](resources/images/AnchorAndPositive.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AnchorAndPNegative](resources/images/AnchorAndNegative.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labeled Face Wild Dataset (LFW)** is a dataset for studying Face Verification. It contains 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. We use this date set as negative images to train the model distinguishing outselves from others. We will explore this dataset in the lecture 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n",
    "\n",
    "# Why need 'join' ? -> Because it will automatically add the correct path separator for the OS like '/' or '\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create those directories in machine\n",
    "os.makedirs(POS_PATH, exist_ok=True) # DO not overwrite if already exists\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 2\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Unpack Labeled Faced in the Wild (LFW) dataset\n",
    "- Collect Anchr images\n",
    "- Collect Positive images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unpack LFW dataset for Negative Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found in this link: https://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Download -> All images as gzipped tar file -> lfw.tgz. Then move to the project workspace and extract the file using:\n",
    "\n",
    "```bash\n",
    "tar -xvzf lfw.tgz\n",
    "```\n",
    "\n",
    "\n",
    "![datasetView1](resources/images/datasetView1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extract, we see the structure of the dataset contains many folders, each folder is a person's name and contains some image(s) of that person inside the folder. We now want to collect all these images in the wholde dataset and put them into our `data/negative` folder. (Only need to take the images, not the folders outside)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment the above code when run the notebook for the first time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the extracted LFW dataset\n",
    "# LFW_PATH = os.path.join('lfw')\n",
    "# #\n",
    "\n",
    "# #  Collect all images from the LFW dataset and move them to the 'data/negative' folder\n",
    "# for dirpath, dirnames, filenames in os.walk(LFW_PATH):\n",
    "#     for filename in filenames:\n",
    "#         if filename.endswith('.jpg'):\n",
    "#             src_path = os.path.join(dirpath, filename)\n",
    "#             dst_path = os.path.join(NEG_PATH, filename)\n",
    "#             os.replace(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this in out `data/negative` folder:\n",
    "\n",
    "![datasetView2](resources/images/datasetView2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Collect Positive and Anchor Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect these images **from our webcam** using OpenCV. The images from LFW have size 250x250, so we will resize the images from our webcam to 250x250 as well.\n",
    "\n",
    "**First, test the webcam:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined the Camera ID to use\n",
    "CAM_ID = 3 # Establishing the connection with the IR camera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CAM_ID = 0 for laptop normal webcam\n",
    "- CAM_ID = 2 for laptop IR webcam\n",
    "- CAM_ID = 4 for external webcam\n",
    "\n",
    "\n",
    "***Depend on each devices, these number can be different. Try out all number start from 0 and see which one is the correct one on your device.***\n",
    "\n",
    "\n",
    "Set up camera:\n",
    "\n",
    "**IR webcam:**\n",
    "\n",
    "What is IR webcam? https://fptshop.com.vn/tin-tuc/danh-gia/ir-camera-la-gi-153147\n",
    "\n",
    "\n",
    "**External webcam**\n",
    "\n",
    "Since the resolution of the laptop webcam is not good, we will use an external webcam like from a mobile phone. To connect the webcam to the laptop:\n",
    "\n",
    "1. Download the DroidCam app on your phone, also the DroidCam client on your laptop: https://www.dev47apps.com/\n",
    "2. Set up as the instruction on the website. With Linux:\n",
    "\n",
    "```bash\n",
    "wget -O droidcam_latest.zip https://files.dev47apps.net/linux/droidcam_2.1.3.zip\n",
    "unzip droidcam_latest.zip -d droidcam\n",
    "cd droidcam && sudo ./install-client\n",
    "sudo apt install libappindicator3-1\n",
    "\n",
    "# Fix missing video device\n",
    "sudo apt install linux-headers-`uname -r` gcc make\n",
    "sudo ./install-video\n",
    "```\n",
    "3. Open both DroidCam on phone and DroidCam client on laptop, connect the phone to the laptop via USB or Wifi.\n",
    "\n",
    "\n",
    "Now try to use OpenCV to capture the image from the webcam:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(CAM_ID)  # Connect to the camera\n",
    "while cap.isOpened():  # Loop through every single frame\n",
    "    ret, frame = cap.read()  # Read the frame\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Image Collection', frame)  # Display the frame to screen\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Wait for 1ms and check if 'q' is pressed\n",
    "        break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()  # Close the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Press `q` to exit**\n",
    "\n",
    "\n",
    "After that, we show to last frame/image captured when press `q`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame) # Display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, OpenCV reads images in **BGR** (Blue, Green, Red) format so the image will look a bit weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.shape # Check the shape of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using normal webcam, the result look something like this:\n",
    "\n",
    "```plaintext\n",
    "array([[[181, 163, 164],\n",
    "        [186, 168, 168],\n",
    "        [186, 164, 168],\n",
    "        ...,\n",
    "        [104, 107, 104],\n",
    "        [104, 106, 106],\n",
    "        [105, 107, 107]],\n",
    "\n",
    "       [[183, 167, 167],\n",
    "        [183, 167, 167],\n",
    "        [180, 162, 163],\n",
    "```\n",
    "Each [181, 163, 164] is a pixel value of the image. The first value is the Blue channel, the second is the Green channel, and the third is the Red channel. The value of each channel is from 0 to 255.\n",
    "\n",
    "Whereas when using IR webcam, the result look something like this:\n",
    "\n",
    "```plaintext\n",
    "array([[[16, 16, 16],\n",
    "        [15, 15, 15],\n",
    "        [23, 23, 23],\n",
    "        ...,\n",
    "        [ 6,  6,  6],\n",
    "        [ 7,  7,  7],\n",
    "        [ 6,  6,  6]],\n",
    "```\n",
    "\n",
    "Each picxel is just a single value, which is the gray intensity of the pixel. The value is from 0 to 255. However they are write duplicated 3 times to match the RGB format, so when use frame.shape, it will return (480, 640, 3) instead of (480, 640, 1).\n",
    "\n",
    "However, this is not 250x250 yet, we will resize the image to 250x250.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize the image to 250x250\n",
    "resized_frame = cv2.resize(frame, (250, 250))\n",
    "plt.imshow(resized_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just resize the image to 250x250 will make the image look weird, so instead, we should crop the image to 250x250, by not by arbitrary, but by the face of the person in the image. We will use the Haar Cascade Classifier to detect the face in the image and crop the image to the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Convert the frame to grayscale as the face detector expects gray images\n",
    "gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "# If faces are detected, crop the first face found\n",
    "if len(faces) > 0:\n",
    "    (x, y, w, h) = faces[0]  # Get the coordinates of the first face\n",
    "    cropped_face = frame[y:y+h, x:x+w]  # Crop the face from the frame\n",
    "\n",
    "    # Resize the cropped face to 250x250\n",
    "    resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "\n",
    "    # Display the resized face\n",
    "    plt.imshow(cv2.cvtColor(resized_face, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No faces detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect images to anchor and positive folders\n",
    "\n",
    "Now we will collect the images to the anchor and positive folders. Create a capture frame, then when press `a`, detect the face in current frame then crop that face to 250x250 image then save to the anchor folder. When press `p`, the image will be saved to the positive folder. notify to the user when the image is saved successfully or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # For generating unique image file names\n",
    "\n",
    "# Function to save the captured image to the specified folder\n",
    "def save_image(image, folder_path, img_name):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    cv2.imwrite(img_path, image)\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(CAM_ID)\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Collection', frame)\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('a'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/anchor'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "            save_image(resized_face, ANC_PATH, str(uuid.uuid1())+ \".jpg\")\n",
    "            print(\"Image saved as anchor\")\n",
    "        else:\n",
    "            # Show a dialog if no faces are detected\n",
    "            print(\"No faces detected\")\n",
    "\n",
    "\n",
    "    elif key == ord('p'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/positive'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "            save_image(resized_face, POS_PATH, str(uuid.uuid1())+ \".jpg\")\n",
    "            print(\"Image saved as positive\")\n",
    "        else:\n",
    "            # Show a dialog if no faces are detectedq\n",
    "            print(\"No faces detected\")\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tim hieu them ve tuat toanFace Detection nay ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 3\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Preprocess the images\n",
    "- Create positive and negative samples\n",
    "- Load data into TensorFlow Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directories to TensorFlow datasets\n",
    "# Take 50 images from each folder\n",
    "anchor = tf.data.Dataset.list_files(os.path.join(ANC_PATH, '*.jpg')).take(50) # This create a pipeline of all images in the folder\n",
    "positive = tf.data.Dataset.list_files(os.path.join(POS_PATH, '*.jpg')).take(50)\n",
    "negative = tf.data.Dataset.list_files(os.path.join(NEG_PATH, '*.jpg')).take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take an example path from `anchor` folder for testing purpose in Preprocess step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an example path from the anchor dataset\n",
    "example_anchor_path = next(iter(anchor))\n",
    "\n",
    "# Print the example path\n",
    "print(example_anchor_path.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    # Read the image from the file path\n",
    "    byte_image = tf.io.read_file(file_path) # Read the image as bytes\n",
    "    image = tf.image.decode_jpeg(byte_image) # Decode the image from bytes to tensor\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = tf.image.resize(image, (100,100)) # Resize the image to 100x100\n",
    "    image = image / 255.0 # Scale image pixel values range [0,1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct dimension to resize will base on the model we use. For example:\n",
    "\n",
    "- VGGFace: 224x224\n",
    "- Facenet: 160x160\n",
    "-....\n",
    "\n",
    "Thu tim them cac model khac va kich thuoc anh phu hop\n",
    "\n",
    "\n",
    "Test the preprocess function with the example path from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images before preprocessing\n",
    "plt.imshow(cv2.imread(example_anchor_path.numpy().decode()))\n",
    "# Value range\n",
    "print(\"Value range before preprocessing:\", frame.min(), \"-\", frame.max())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Images after preprocessing\n",
    "preprocessed_image_test = preprocess(example_anchor_path)\n",
    "print(\"Value range before preprocessing:\", preprocessed_image_test.numpy().min(), \"-\", preprocessed_image_test.numpy().max())\n",
    "plt.imshow(preprocessed_image_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create labled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a dataset that labels as follow:\n",
    "\n",
    "- Given two input: Anchor + Positive => 1\n",
    "- Given two input: Anchor + Negative => 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "positive_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))) #each label has the same size as the anchor\n",
    "negative_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))\n",
    "\n",
    "# Zip the datasets with their labels\n",
    "positives = tf.data.Dataset.zip((anchor, positive, positive_labels))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, negative_labels))\n",
    "\n",
    "# Concatenate the datasets\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 examples\n",
    "# Print the dataset to verify\n",
    "for element in data.take(5):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a sample from the dataset\n",
    "samples = data.as_numpy_iterator()\n",
    "samples.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Build Train and Test Parition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `preprocess` function to each sample in the dataset. Curretly, the `preprocess` function only precess one passed image, so we create a new function to process two images at the same time (just calling the `preprocess` function twice inside that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    input_img = preprocess(input_img)\n",
    "    valid_img = preprocess(validation_img)\n",
    "    return input_img, valid_img, label\n",
    "\n",
    "# input_img will be the anchor image, valid_img will be the positive or negative image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample\n",
    "samples = data.as_numpy_iterator()\n",
    "sample = samples.next()\n",
    "\n",
    "res = preprocess_twin(*sample) # `*` is used to unpack the tuple\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the result in image format to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anchor image: \")\n",
    "plt.imshow(res[0])\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation image: \")\n",
    "plt.imshow(res[1])\n",
    "plt.show()\n",
    "\n",
    "# Directly print the value of res[2]\n",
    "print(\"Label:\", res[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tesing all good for a single sample, we apply this process to all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache() # Cache the dataset to memory to get a speedup while reading from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the dataset contains **positive classes consecutively, then negative classes consecutively**. We need to shuffle the dataset to make it more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before mix\n",
    "for element in data.take(10):\n",
    "    print(element[2].numpy())  # Print the label to see the mix of classes\n",
    "# See if the appearance of 0 and 1 is random yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset so that we have a good mix of positive and negative examples\n",
    "data = data.shuffle(buffer_size=1024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset again to see that the two classes are mixed well\n",
    "for element in data.take(10):\n",
    "    print(element[2].numpy())  # Print the label to see the mix of classes\n",
    "# See if the appearance of 0 and 1 is random yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the Dataset in to Training and Testing set. We will use **70% for training and 30% for testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.take(round(len(data)*0.7)) # 70% of the data for training\n",
    "train_data = train_data.batch(16) # Batch size of 16; batch size\" refers to the number of training examples utilized in one iteration of the training process.\n",
    "train_data = train_data.prefetch(8) # start processing the next batch while the current batch is being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of original dataset:\", len(data))\n",
    "print(\"Size of training dataset:\", len(list(train_data.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that now each sample in train_data is 16 images\n",
    "for element in train_data.take(1):\n",
    "    print(f\"Size of each sample in train_data: {element[0].shape}, {element[1].shape}, {element[2].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(16, 100, 100, 3) represents the anchor images in one single sample:\n",
    "\n",
    "- 16: meaning there are 16 images in this batch.\n",
    "- 100, 100: the height and width of each image, indicating that each image is 100 pixels by 100 pixels.\n",
    "- 3: the number of color channels in each image. We store the image as Gray scale, but the vluae is duplicated 3 times to match the RGB format.\n",
    "\n",
    "Similiarly for the remaining two elements in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape and values of element[0][0] (the first image in the first batch) to verify if it is stored as a grayscale image\n",
    "print(\"Shape of element[0][0]:\", element[0][0].shape)\n",
    "print(\"Values of element[0][0]:\")\n",
    "print(element[0][0].numpy())\n",
    "\n",
    "plt.imshow(element[0][0]) # Display the first image in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do similar for the test data 30%\n",
    "test_data = data.skip(round(len(data)*0.7)) # Skip the first 70% of the data which is used for training\n",
    "test_data = test_data.take(round(len(data)*0.3)) \n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of original dataset:\", len(data))\n",
    "print(\"Size of training dataset:\", len(list(train_data.as_numpy_iterator())))\n",
    "print(\"Size of test dataset:\", len(list(test_data.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 4\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Build an embedding layer / Encode Layer\n",
    "- Create an L1 Distance layer\n",
    "- Complie the Siamase Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build an Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall th idea, we have two stream of information (we  pass in two images: the anchor and the positive/negative image) -> Each stream will pass through an embedding layer to get the feature vector of the image -> Tow feature vectors will be used to calculate the distance between the two images at the Distance layer (here we use L1 distance).\n",
    "\n",
    "\n",
    "Here is the Siamese Network architecture we are going to build:\n",
    "\n",
    "![SiameseStructure](resources/images/SiameseStructure.png)\n",
    "\n",
    "In the paper, it use input size of 105x105, but we will use 100x100 still okey. Therefore, **some numbers may not match exactly to the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding() :\n",
    "\n",
    "    #Create the input layer\n",
    "    inp = Input(shape=(100, 100, 3), name='input_image') # 100x100 image size, 3 channels color\n",
    "\n",
    "   \n",
    "    ### First Block ###\n",
    "    # Next layer is a convolutional layer 64 filters, kernel size of 10x10, and ReLU activation\n",
    "    c1= Conv2D(64, (10,10), activation='relu')(inp) # c1 stands for convolutional layer 1\n",
    "\n",
    "    # Next layer is a max pooling layer with a pool size of 2x2\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1) \n",
    "\n",
    "\n",
    "\n",
    "    ### Second Block ###\n",
    "    # Next layer is a convolutional layer 128 filters, kernel size of 7x7, and ReLU activation\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    # Next layer is a max pooling layer with a pool size of 2x2\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "\n",
    "\n",
    "\n",
    "    ### Third Block ###\n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "\n",
    "    ### Fourth Block ###\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4) # Flatten the output of the convolutional layer to feed it to the dense layer\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1) # Dense layer with 4096 neurons and sigmoid activation\n",
    "\n",
    "    \n",
    "    return Model(inputs=inp, outputs=d1, name='Siamese__embedding_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As show in the above image, the output of the Embedding layer is a 4096-dimensional feature vector. Two streams of information will pass through this Embedding layer then we get two 4096-dimensional feature vectors for each image. **Basicaly, we are converting the face image to a 4096-dimensional feature vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model\n",
    "siamese_embedding_model = make_embedding()\n",
    "siamese_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read from the summary table:\n",
    "\n",
    "This is a 4-dimensional tensor shape where each dimension represents, for example (None,100,100,3):\n",
    "\n",
    "- None: This represents the batch size dimension. It's set to \"None\" because it's flexible - you can feed any number of images through the network at once.\n",
    "- 100: The height of your input images (100 pixels)\n",
    "- 100: The width of your input images (100 pixels)\n",
    "- 3: The number of color channels\n",
    "\n",
    "So in plain terms, your model is expecting input images that are:\n",
    "\n",
    "- 100x100 pixels in size\n",
    "- Color in 3 channels, can be RGB, BRG or even just grayscale but duplicated 3 times to match the format.\n",
    "- Can be processed in batches of any size (that's what the None indicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create an L1 Distance Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two feature vectors to see how similiar our two images are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class L1Dist(Layer):\n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "\n",
    "        # Convert inputs to tensors otherwise will meet error: un supported operand type(s) for -: 'List' and 'List'\n",
    "        input_embedding = tf.convert_to_tensor(input_embedding)\n",
    "        validation_embedding = tf.convert_to_tensor(validation_embedding)\n",
    "        input_embedding = tf.squeeze(input_embedding, axis=0)  # Remove potential first dimension\n",
    "        validation_embedding = tf.squeeze(validation_embedding, axis=0)\n",
    "\n",
    "        # Calculate and return the L1 distance\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create final Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine two above steps to make a fully Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "\n",
    "    \n",
    "    # Handle input images\n",
    "    input_image = Input(shape=(100, 100, 3), name='input_image') # Anchor image\n",
    "    validation_image = Input(shape=(100, 100, 3), name='validation_image') # Positive or negative image\n",
    "\n",
    "\n",
    "    # Pass through the embedding model/Encoder layer to get the features vector\n",
    "    embedding = make_embedding()\n",
    "    input_embedding = embedding(input_image)\n",
    "    validation_embedding = embedding(validation_image)\n",
    "\n",
    "\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(input_embedding, validation_embedding)\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='Fully_Siamese_Network')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "fully_siamese_model = make_siamese_model()\n",
    "fully_siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the summary table, we see that the input of the model is two images size 100,100,3. (How many images per batch is not specified, so it's flexible). The output is a **single value**, which is the distance between the two images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train the Siamese Network\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Set up a loss function, set up an optimizer\n",
    "- Establish checkpoints\n",
    "- Build a Custom Training Step\n",
    "- Create a Training Loop\n",
    "- Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Set up a loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function\n",
    "binary_cross_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# from_logits=True is recommened when thr inputs to the loss function are not normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are tons of optimizer for Keras: SGD, Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Ftrl. We will use Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Establish checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checkpoint will save the model after each epoch. In case the training process is interrupted, we can resume the training from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training_checkpoints directory in th machine\n",
    "os.makedirs('./training_checkpoints', exist_ok=True)\n",
    "\n",
    "\n",
    "checkpoint_dir = './training_checkpoints' # Directory to save the checkpoints\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\") #Prefix for the checkpoint files with unique number\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, siamese_model=fully_siamese_model)\n",
    "\n",
    "# To reload the model from the checkpoint, use model.load('path_to_checkpoint'). This will\n",
    "# load pre trainweights and optimizer state into existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build a Custom Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we defined actual trainning steps. We train on one batch of data, one batch of data come through our training step, we go on makeing prediction -> calcualte our loss function -> calculate gradient then apply back popagation (calculate new weights and apply) through our neutral network to get the best possible model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # Annotation to indicate that complies to TensorFlow graph execution \n",
    "def train_step(batch):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        \n",
    "        # Get the input images, validation images and labels from a batch\n",
    "        input_images, validation_images, labels = batch\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = fully_siamese_model([input_images, validation_images], training=True)\n",
    "        # Set training=True for is important since some layers will only activated when this is set to True\n",
    "    \n",
    "    \n",
    "        # Calculate the loss\n",
    "        loss = binary_cross_loss(labels, predictions) # Measure the difference between the predicted value and the actual value\n",
    "        print(\"Loss Vaule: \", loss)\n",
    "    \n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(loss, fully_siamese_model.trainable_variables)\n",
    "\n",
    "        # Update the weights and apply to the Siamese model\n",
    "        optimizer.apply_gradients(zip(gradients, fully_siamese_model.trainable_variables))\n",
    "        # Adam is a variant of stochastic gradient descent, it applies te learning rate and gradient to slightly reduce the loss function, unitll\n",
    "        # it realy near the minimum value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the batch we created before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take one testing batch from train_data\n",
    "test_batch = train_data.as_numpy_iterator()\n",
    "test_batch = test_batch.next()\n",
    "print(\"The length of the batch is\", len(test_batch))\n",
    "print(\"The first element of the batch is 16 anchor images: \", test_batch[0].shape)\n",
    "print(\"The second element of the batch is 16 validation images: \", test_batch[1].shape)\n",
    "print(\"The third element of the batch is a single label 1 or 0 (but write duplicated 16 times): \", test_batch[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS): # EPOCHS is the number of times the model will see the entire dataset\n",
    "\n",
    "    # Loop through the epochs\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS)) # Print the current epoch to know the progress of runnig\n",
    "        progressBar = tf.keras.utils.Progbar(len(data))\n",
    "\n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run the training step here\n",
    "            train_step(batch)\n",
    "            progressBar.update(idx+1) # Update the progress bar\n",
    "\n",
    "    # Save our checkpoint \n",
    "    if (epoch) % 10 == 0: # Save the model every 10 epochs\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "EPOCHS = 20\n",
    "\n",
    "train(train_data, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

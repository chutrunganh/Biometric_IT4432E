{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 1\n",
    "\n",
    "## Objective\n",
    "\n",
    "General setup:\n",
    "\n",
    "- Install dependencies\n",
    "- Import TensorFlow Layers\n",
    "- Config GPU\n",
    "- Create Data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install and import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a virtual environment for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%python3 -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "#%source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import some necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV\n",
    "import os  # For file operations\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # For plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D,  Dense, MaxPool2D, Flatten, Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv2D for Convolutional Neural Networks\n",
    "- Dense for Fully Connected Neural Networks\n",
    "- MaxPooling2D to pull our layers together and effectively reduce the number of parameters and computations in the network\n",
    "- Flatten take output from previous layer and flatten it to a vector to be fed into the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Setup GPU Limit for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid Memory Error when TensorFlow use two much VRAM\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus # Show all GPUs available in machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create Data Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform facial verification, we actually want to compare two images and determine if they are of the same person. This is a one-shot learning problem. We will learn a function that maps our input data (images) to a feature space such that the distance between the features can be used to determine if the images are of the same person.\n",
    "\n",
    "We will create three data folders:\n",
    "\n",
    "- anchors\n",
    "- positive\n",
    "- negative\n",
    "\n",
    "The anchor image is the image of the person we want to verify.(Input)\n",
    "The positive or negative are verification images. The positive is an image of the same person as the anchor image but taken under different conditions. The negative image is an image of a different person.\n",
    "\n",
    "When we pass in a anchor image, if:\n",
    "\n",
    "- it matches the positive image, the output should be 1 (same)\n",
    "- it matches the negative image, the output should be 0 (different)\n",
    "\n",
    "We want our model to distinguish between outselves and others person. The process will be as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix spelling later :)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AnchorAndPositive](resources/images/AnchorAndPositive.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AnchorAndPNegative](resources/images/AnchorAndNegative.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labeled Face Wild Dataset (LFW)** is a dataset for studying Face Verification. It contains 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. We use this date set as negative images to train the model distinguishing outselves from others. We will explore this dataset in the lecture 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n",
    "\n",
    "# Why need 'join' ? -> Because it will automatically add the correct path separator for the OS like '/' or '\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create those directories in machine\n",
    "os.makedirs(POS_PATH, exist_ok=True) # DO not overwrite if already exists\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 2\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Unpack Labeled Faced in the Wild (LFW) dataset\n",
    "- Collect Anchr images\n",
    "- Collect Positive images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unpack LFW dataset for Negative Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found in this link: https://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Download -> All images as gzipped tar file -> lfw.tgz. Then move to the project workspace and extract the file using:\n",
    "\n",
    "```bash\n",
    "tar -xvzf lfw.tgz\n",
    "```\n",
    "\n",
    "\n",
    "![datasetView1](resources/images/datasetView1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extract, we see the structure of the dataset contains many folders, each folder is a person's name and contains some image(s) of that person inside the folder. We now want to collect all these images in the wholde dataset and put them into our `data/negative` folder. (Only need to take the images, not the folders outside)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment the above code when run the notebook for the first time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the extracted LFW dataset\n",
    "# LFW_PATH = os.path.join('lfw')\n",
    "# #\n",
    "\n",
    "# #  Collect all images from the LFW dataset and move them to the 'data/negative' folder\n",
    "# for dirpath, dirnames, filenames in os.walk(LFW_PATH):\n",
    "#     for filename in filenames:\n",
    "#         if filename.endswith('.jpg'):\n",
    "#             src_path = os.path.join(dirpath, filename)\n",
    "#             dst_path = os.path.join(NEG_PATH, filename)\n",
    "#             os.replace(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this in out `data/negative` folder:\n",
    "\n",
    "![datasetView2](resources/images/datasetView2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Collect Positive and Anchor Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect these images **from our webcam** using OpenCV. The images from LFW have size 250x250, so we will resize the images from our webcam to 250x250 as well.\n",
    "\n",
    "**First, test the webcam:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(2)  # Establishing the connection with the camera 2 (IR camera)\n",
    "\n",
    "while cap.isOpened():  # Loop through every single frame\n",
    "    ret, frame = cap.read()  # Read the frame\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Image Collection', frame)  # Display the resized frame to screen\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Wait for 1ms and check if 'q' is pressed\n",
    "        break\n",
    "\n",
    "cap.release()  # R# Define the path to the extracted LFW dataset\n",
    "# LFW_PATH = os.path.join('lfw')\n",
    "# #\n",
    "\n",
    "# #  Collect all images from the LFW dataset and move them to the 'data/negative' folder\n",
    "# for dirpath, dirnames, filenames in os.walk(LFW_PATH):\n",
    "#     for filename in filenames:\n",
    "#         if filename.endswith('.jpg'):\n",
    "#             src_path = os.path.join(dirpath, filename)\n",
    "#             dst_path = os.path.join(NEG_PATH, filename)\n",
    "#             os.replace(src_path, dst_path)qelease the camera\n",
    "cv2.destroyAllWindows()  # Close the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Press `q` to exit**\n",
    "\n",
    "- cap = cv2.VideoCapture(0)  for normal webcam\n",
    "\n",
    "- cap = cv2.VideoCapture(2) for IR webcam\n",
    "\n",
    "- cap = cv2.VideoCapture(4) for external webcam\n",
    "\n",
    "Depend on each devices, these number can be different. Try out all number start from 0 and see which one is the correct one on your device.\n",
    "\n",
    "**IR webcam:**\n",
    "\n",
    "What is IR webcam? https://fptshop.com.vn/tin-tuc/danh-gia/ir-camera-la-gi-153147\n",
    "\n",
    "\n",
    "**External webcam**\n",
    "\n",
    "Since the resolution of the laptop webcam is not good, we will use an external webcam like from a mobile phone. To connect the webcam to the laptop:\n",
    "\n",
    "1. Download the DroidCam app on your phone, also the DroidCam client on your laptop: https://www.dev47apps.com/\n",
    "2. Set up as the instruction on the website. With Linux:\n",
    "\n",
    "```bash\n",
    "wget -O droidcam_latest.zip https://files.dev47apps.net/linux/droidcam_2.1.3.zip\n",
    "unzip droidcam_latest.zip -d droidcam\n",
    "cd droidcam && sudo ./install-client\n",
    "sudo apt install libappindicator3-1\n",
    "\n",
    "# Fix missing video device\n",
    "sudo apt install linux-headers-`uname -r` gcc make\n",
    "sudo ./install-video\n",
    "```\n",
    "3. Open both DroidCam on phone and DroidCam client on laptop, connect the phone to the laptop via USB or Wifi.\n",
    "\n",
    "\n",
    "After that, we show to last frame/image captured when press `q`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame) # Display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, OpenCV reads images in **BGR** (Blue, Green, Red) format so the image will look a bit weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.shape # Check the shape of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using normal webcam, the result look something like this:\n",
    "\n",
    "```plaintext\n",
    "array([[[181, 163, 164],\n",
    "        [186, 168, 168],\n",
    "        [186, 164, 168],\n",
    "        ...,\n",
    "        [104, 107, 104],\n",
    "        [104, 106, 106],\n",
    "        [105, 107, 107]],\n",
    "\n",
    "       [[183, 167, 167],\n",
    "        [183, 167, 167],\n",
    "        [180, 162, 163],\n",
    "```\n",
    "Each [181, 163, 164] is a pixel value of the image. The first value is the Blue channel, the second is the Green channel, and the third is the Red channel. The value of each channel is from 0 to 255.\n",
    "\n",
    "Whereas when using IR webcam, the result look something like this:\n",
    "\n",
    "```plaintext\n",
    "array([[[16, 16, 16],\n",
    "        [15, 15, 15],\n",
    "        [23, 23, 23],\n",
    "        ...,\n",
    "        [ 6,  6,  6],\n",
    "        [ 7,  7,  7],\n",
    "        [ 6,  6,  6]],\n",
    "```\n",
    "\n",
    "Each picxel is just a single value, which is the gray intensity of the pixel. The value is from 0 to 255. However they are write duplicated 3 times to match the RGB format, so when use frame.shape, it will return (480, 640, 3) instead of (480, 640, 1).\n",
    "\n",
    "However, this is not 250x250 yet, we will resize the image to 250x250.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize the image to 250x250\n",
    "resized_frame = cv2.resize(frame, (250, 250))\n",
    "plt.imshow(resized_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just resize the image to 250x250 will make the image look weird, so instead, we should crop the image to 250x250, by not by arbitrary, but by the face of the person in the image. We will use the Haar Cascade Classifier to detect the face in the image and crop the image to the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Convert the frame to grayscale as the face detector expects gray images\n",
    "gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "# If faces are detected, crop the first face found\n",
    "if len(faces) > 0:\n",
    "    (x, y, w, h) = faces[0]  # Get the coordinates of the first face\n",
    "    cropped_face = frame[y:y+h, x:x+w]  # Crop the face from the frame\n",
    "\n",
    "    # Resize the cropped face to 250x250\n",
    "    resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "\n",
    "    # Display the resized face\n",
    "    plt.imshow(cv2.cvtColor(resized_face, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No faces detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect images to anchor and positive folders\n",
    "\n",
    "Now we will collect the images to the anchor and positive folders. Create a capture frame, then when press `a`, detect the face in current frame then crop that face to 250x250 image then save to the anchor folder. When press `p`, the image will be saved to the positive folder. notify to the user when the image is saved successfully or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # For generating unique image file names\n",
    "\n",
    "# Function to save the captured image to the specified folder\n",
    "def save_image(image, folder_path, img_name):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    cv2.imwrite(img_path, image)\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(2)  # Use 0 for normal webcam, 2 for IR webcam\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Collection', frame)\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('a'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/anchor'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "            save_image(resized_face, ANC_PATH, str(uuid.uuid1())+ \".jpg\")\n",
    "            print(\"Image saved as anchor\")\n",
    "        else:\n",
    "            # Show a dialog if no faces are detected\n",
    "            print(\"No faces detected\")\n",
    "\n",
    "\n",
    "    elif key == ord('p'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/positive'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "            save_image(resized_face, POS_PATH, str(uuid.uuid1())+ \".jpg\")\n",
    "            print(\"Image saved as positive\")\n",
    "        else:\n",
    "            # Show a dialog if no faces are detectedq\n",
    "            print(\"No faces detected\")\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tim hieu them ve tuat toanFace Detection nay ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 3\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Preprocess the images\n",
    "- Create positive and negative samples\n",
    "- Load data into TensorFlow Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directories to TensorFlow datasets\n",
    "# Take 50 images from each folder\n",
    "anchor = tf.data.Dataset.list_files(os.path.join(ANC_PATH, '*.jpg')).take(50) # This create a pipeline of all images in the folder\n",
    "positive = tf.data.Dataset.list_files(os.path.join(POS_PATH, '*.jpg')).take(50)\n",
    "negative = tf.data.Dataset.list_files(os.path.join(NEG_PATH, '*.jpg')).take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take an example path from `anchor` folder for testing purpose in Preprocess step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an example path from the anchor dataset\n",
    "example_anchor_path = next(iter(anchor))\n",
    "\n",
    "# Print the example path\n",
    "print(example_anchor_path.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    # Read the image from the file path\n",
    "    byte_image = tf.io.read_file(file_path) # Read the image as bytes\n",
    "    image = tf.image.decode_jpeg(byte_image) # Decode the image from bytes to tensor\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = tf.image.resize(image, (100,100)) # Resize the image to 100x100\n",
    "    image = image / 255.0 # Scale image pixel values range [0,1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct dimension to resize will base on the model we use. For example:\n",
    "\n",
    "- VGGFace: 224x224\n",
    "- Facenet: 160x160\n",
    "-....\n",
    "\n",
    "Thu tim them cac model khac va kich thuoc anh phu hop\n",
    "\n",
    "\n",
    "Test the preprocess function with the example path from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images before preprocessing\n",
    "plt.imshow(cv2.imread(example_anchor_path.numpy().decode()))\n",
    "# Value range\n",
    "print(\"Value range before preprocessing:\", frame.min(), \"-\", frame.max())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Images after preprocessing\n",
    "preprocessed_image_test = preprocess(example_anchor_path)\n",
    "print(\"Value range before preprocessing:\", preprocessed_image_test.numpy().min(), \"-\", preprocessed_image_test.numpy().max())\n",
    "plt.imshow(preprocessed_image_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create labled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a dataset that labels as follow:\n",
    "\n",
    "- Given two input: Anchor + Positive => 1\n",
    "- Given two input: Anchor + Negative => 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "positive_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))) #each label has the same size as the anchor\n",
    "negative_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))\n",
    "\n",
    "# Zip the datasets with their labels\n",
    "positives = tf.data.Dataset.zip((anchor, positive, positive_labels))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, negative_labels))\n",
    "\n",
    "# Concatenate the datasets\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 examples\n",
    "# Print the dataset to verify\n",
    "for element in data.take(5):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a sample from the dataset\n",
    "samples = data.as_numpy_iterator()\n",
    "samples.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Build Train and Test Parition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `preprocess` function to each sample in the dataset. Curretly, the `preprocess` function only precess one passed image, so we create a new function to process two images at the same time (just calling the `preprocess` function twice inside that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    input_img = preprocess(input_img)\n",
    "    valid_img = preprocess(validation_img)\n",
    "    return input_img, valid_img, label\n",
    "\n",
    "# input_img will be the anchor image, valid_img will be the positive or negative image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample\n",
    "samples = data.as_numpy_iterator()\n",
    "sample = samples.next()\n",
    "\n",
    "res = preprocess_twin(*sample) # `*` is used to unpack the tuple\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the result in image format to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anchor image: \")\n",
    "plt.imshow(res[0])\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation image: \")\n",
    "plt.imshow(res[1])\n",
    "plt.show()\n",
    "\n",
    "# Directly print the value of res[2]\n",
    "print(\"Label:\", res[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tesing all good for a single sample, we apply this process to all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache() # Cache the dataset to memory to get a speedup while reading from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the dataset contains **positive classes consecutively, then negative classes consecutively**. We need to shuffle the dataset to make it more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before mix\n",
    "for element in data.take(10):\n",
    "    print(element[2].numpy())  # Print the label to see the mix of classes\n",
    "# See if the appearance of 0 and 1 is random yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset so that we have a good mix of positive and negative examples\n",
    "data = data.shuffle(buffer_size=1024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset again to see that the two classes are mixed well\n",
    "for element in data.take(10):\n",
    "    print(element[2].numpy())  # Print the label to see the mix of classes\n",
    "# See if the appearance of 0 and 1 is random yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the Dataset in to Training and Testing set. We will use **70% for training and 30% for testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.take(round(len(data)*0.7)) # 70% of the data for training\n",
    "train_data = train_data.batch(16) # Batch size of 16; batch size\" refers to the number of training examples utilized in one iteration of the training process.\n",
    "train_data = train_data.prefetch(8) # start processing the next batch while the current batch is being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of original dataset:\", len(data))\n",
    "print(\"Size of training dataset:\", len(list(train_data.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that now each sample in train_data is 16 images\n",
    "for element in train_data.take(1):\n",
    "    print(f\"Size of each sample in train_data: {element[0].shape}, {element[1].shape}, {element[2].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(16, 100, 100, 3) represents the anchor images in one single sample:\n",
    "\n",
    "- 16: meaning there are 16 images in this batch.\n",
    "- 100, 100: the height and width of each image, indicating that each image is 100 pixels by 100 pixels.\n",
    "- 3: the number of color channels in each image. We store the image as Gray scale, but the vluae is duplicated 3 times to match the RGB format.\n",
    "\n",
    "Similiarly for the remaining two elements in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape and values of element[0][0] (the first image in the first batch) to verify if it is stored as a grayscale image\n",
    "print(\"Shape of element[0][0]:\", element[0][0].shape)\n",
    "print(\"Values of element[0][0]:\")\n",
    "print(element[0][0].numpy())\n",
    "\n",
    "plt.imshow(element[0][0]) # Display the first image in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do similar for the test data 30%\n",
    "test_data = data.skip(round(len(data)*0.7)) # Skip the first 70% of the data which is used for training\n",
    "test_data = test_data.take(round(len(data)*0.3)) \n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of original dataset:\", len(data))\n",
    "print(\"Size of training dataset:\", len(list(train_data.as_numpy_iterator())))\n",
    "print(\"Size of test dataset:\", len(list(test_data.as_numpy_iterator())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

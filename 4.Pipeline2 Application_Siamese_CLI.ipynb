{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "In this notebook, we use the Siamese Network we trainied in `Pipeline2 Siamese_Network.ipynb` to build an CLI application that can be used to recognize faces in real time.\n",
    "\n",
    "\n",
    "\n",
    "- Setup verification images/ Enrollment process\n",
    "\n",
    "- Set up Login process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import all dependencies, needed custom functions "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:39.770374Z",
     "start_time": "2024-12-22T05:12:37.041117Z"
    }
   },
   "source": [
    "import cv2 # OpenCV\n",
    "import os  # For file operations\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # For plotting graphs\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import savez_compressed\n",
    "from PIL import Image\n",
    "\n",
    "# Import TensorFlow dependencies\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D,  Dense, MaxPool2D, Flatten, Input\n",
    "import tensorflow as tf\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:39.776833Z",
     "start_time": "2024-12-22T05:12:39.772711Z"
    }
   },
   "source": [
    "# Defined the Camera ID to use\n",
    "CAM_ID = 0 # Establishing the connection with the IR camera"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Siamese nodel  need some custom function, Layer that we defined in the building phase.  They are not inside the model itself, so we need to import them. These functions are just copy paste from `Pipeline2 Siamese_Network.ipynb` and `Pipeline2 Data_Preparation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:39.875368Z",
     "start_time": "2024-12-22T05:12:39.867444Z"
    }
   },
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "         super(L1Dist, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,input_embedding, validation_embedding):\n",
    "        \n",
    "        # Convert inputs to tensors otherwise will meet error: unsupported operand type(s) for -: 'List' and 'List'\n",
    "        input_embedding = tf.convert_to_tensor(input_embedding)\n",
    "        validation_embedding = tf.convert_to_tensor(validation_embedding)\n",
    "        input_embedding = tf.squeeze(input_embedding, axis=0)  # Remove potential first dimension\n",
    "        validation_embedding = tf.squeeze(validation_embedding, axis=0)\n",
    "\n",
    "        # Calculate and return the L1 distance\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:40.577742Z",
     "start_time": "2024-12-22T05:12:39.892394Z"
    }
   },
   "source": [
    "## Load the model from the saved file ##\n",
    "import tensorflow as tf\n",
    "# Reload the model\n",
    "model = tf.keras.models.load_model('model_saved/fully_siamese_network.h5', \n",
    "                                   custom_objects={'L1Dist': L1Dist, 'BinaryCrossentropy': tf.losses.BinaryCrossentropy},\n",
    "                                   compile=False)\n",
    "# Without complie=false cause Warning: WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chu Trung Anh\\Desktop\\Biometric_IT4432E\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:40.587505Z",
     "start_time": "2024-12-22T05:12:40.582258Z"
    }
   },
   "source": [
    "def gaussian_blur(image, kernel_size=(3,3), sigma=0.1):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur to an image using TensorFlow with auto-determined sigma.\n",
    "    \n",
    "    Args:\n",
    "    - image: Input image tensor\n",
    "    - kernel_size: Size of the Gaussian kernel (height, width)\n",
    "    \n",
    "    Returns:\n",
    "    - Smoothed image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the image is a tensor\n",
    "    if not isinstance(image, tf.Tensor):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "    \n",
    "    # Ensure 4D tensor [batch, height, width, channels]\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[tf.newaxis, :, :, :]\n",
    "    \n",
    "    # Create Gaussian kernel for each channel\n",
    "    def create_gaussian_kernel(size, sigma=1.0):\n",
    "        \"\"\"Generate a 2D Gaussian kernel\"\"\"\n",
    "        size = int(size)\n",
    "        x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "        g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "        return g / g.sum()\n",
    "    \n",
    "    # Create kernel\n",
    "    kernel_height, kernel_width = kernel_size\n",
    "    kernel = create_gaussian_kernel(kernel_height, sigma)\n",
    "    \n",
    "    # Expand kernel for all channels\n",
    "    num_channels = image.shape[-1]\n",
    "    kernel_4d = np.expand_dims(kernel, axis=-1)\n",
    "    kernel_4d = np.repeat(kernel_4d, num_channels, axis=-1)\n",
    "    kernel_4d = np.expand_dims(kernel_4d, axis=-1)\n",
    "    \n",
    "    # Convert kernel to float32 tensor\n",
    "    kernel_tensor = tf.convert_to_tensor(kernel_4d, dtype=tf.float32)\n",
    "    \n",
    "    # Apply convolution\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "        input=image, \n",
    "        filter=kernel_tensor, \n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding='SAME'\n",
    "    )\n",
    "    \n",
    "    # Remove batch dimension if it was added\n",
    "    return blurred[0] if blurred.shape[0] == 1 else blurred"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:40.601149Z",
     "start_time": "2024-12-22T05:12:40.593721Z"
    }
   },
   "source": [
    "def preprocess(input_data):\n",
    "    \"\"\"\n",
    "    Preprocess image data from various input formats into a standardized tensor.\n",
    "    \n",
    "    Args:\n",
    "    input_data: Can be a file path (str), bytes tensor, numpy array, or PIL Image\n",
    "    \n",
    "    Returns:\n",
    "    A preprocessed tensor of shape (100, 100, 3) with values in [0,1]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle PIL Image input\n",
    "        if isinstance(input_data, Image.Image):\n",
    "            input_data = np.array(input_data)\n",
    "        \n",
    "        # Image decoding and initial processing\n",
    "        if isinstance(input_data, (str, bytes)) or (isinstance(input_data, tf.Tensor) and input_data.dtype == tf.string):\n",
    "            # Convert tensor to string if needed\n",
    "            if isinstance(input_data, tf.Tensor):\n",
    "                input_data = input_data.numpy()\n",
    "            if isinstance(input_data, bytes):\n",
    "                input_data = input_data.decode('utf-8')\n",
    "            \n",
    "            # Read and decode the image\n",
    "            byte_image = tf.io.read_file(input_data)\n",
    "            image = tf.image.decode_jpeg(byte_image, channels=3)\n",
    "        else:\n",
    "            # Handle numpy array or TensorFlow tensor input\n",
    "            image = tf.convert_to_tensor(input_data)\n",
    "        \n",
    "        # Convert to float32\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        \n",
    "        # Ensure shape is correct\n",
    "        if len(image.shape) != 3:\n",
    "            raise ValueError(f\"Expected image with 3 dimensions, got shape {image.shape}\")\n",
    "        \n",
    "        # Resize the image\n",
    "        image = tf.image.resize(image, (100, 100))\n",
    "        \n",
    "        # Smooth the image\n",
    "        image = gaussian_blur(image, kernel_size=(3,3), sigma=0.1)\n",
    "        \n",
    "        # Normalize the image\n",
    "        # WIth deep learing, it is ensential to normalize, so   can improve model \n",
    "        # performance by ensuring that input data is within a smaller, consistent range, which can help with stability during training.\n",
    "        image = image / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        '''\n",
    "        However, scaling might make the image look lower quality because of the smaller numerical range (0-1), even though \n",
    "        this does not actually affect its visual structure when used in a deep learning model. This step is not \n",
    "        meant for direct visualization, but rather for preparing data for model input.\n",
    "\n",
    "        If you are trying to visually inspect the image to verify it after scaling, you can:\n",
    "        '''\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        print(f\"Input type: {type(input_data)}\")\n",
    "        if isinstance(input_data, (str, bytes)):\n",
    "            print(f\"Input path: {input_data}\")\n",
    "        raise\n",
    "# Note that our preprocess function return a Tensorflow tensor, not a numpy array, so when need  to  perform image \n",
    "# with OpenCV, we need to convert it to numpy array\n",
    "\n",
    "# Wrap the preprocess function in a tf.py_function to deal with Frame objects in Opencv\n",
    "def preprocess_wrapper(input_data):\n",
    "    \"\"\"Wrapper function to use with tf.py_function if needed\"\"\"\n",
    "    return tf.py_function(preprocess, [input_data], tf.float32)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overall process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Enrollment**\n",
    "\n",
    "Collect the personal images of that person for verification/enrollment process through webcam. It is similar to when you first choose the sign-in option in Windows Hello, where you need to scan your face images for the first time. These images will be stored to compare with the input image each time you log in to the computer later.\n",
    "\n",
    "2. **Verification/Login**\n",
    "\n",
    "Access webcam -> retrieve input image of user when they want to log in (this input image is processed directly without being stored to any file) -> use this image to verify against a number of positive samples (these positive samples are images already collected as part of our enrollment process). We store the positive samples or called validation images inside the `application_data/verification_images` folder. Each user on the system will have their own subfolder inside the `verification_images` folder.\n",
    "\n",
    "With an input image, loop to compare against all, for example, 50 positive images in `validation_images/user_name` folder -> Our verification function will output 50 predictions. So for example, an input image + one verification image (1 of 50 images in folder) will be compared, and the output will be a number between 0 and 1. We must choose a threshold to determine if the input image is a match or not (**detection threshold**). After that, we get 50 results of matching or not matching. Then we choose a **Verification threshold** to determine the number of matching images out of 50 to be considered a valid authentication. For example, choose the threshold to be 0.8, meaning that if 80% of the 50 images match the input image, then we consider the input image to be a match.\n",
    "\n",
    "\n",
    "![VerificationProcess](assets/images/VerificationProcess.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:40.612445Z",
     "start_time": "2024-12-22T05:12:40.606951Z"
    }
   },
   "source": [
    "# Create base directory\n",
    "base_dir = 'application_data'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Create validation images directory\n",
    "validation_dir = os.path.join(base_dir, 'validation_images')\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Enrollment process"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:54.561128Z",
     "start_time": "2024-12-22T05:12:40.618717Z"
    }
   },
   "source": [
    "# Prompt user to enter their name  to save the verifaction images of that person, then create\n",
    "# a folder with the name of the person in the validation_images folder\n",
    "name = input(\"Enter your name to store your personal verification data to system: \")\n",
    "print(\"enrollment process begins, please look at the camera, rotate your head to the left and right\")\n",
    "print(\"Press 'p' to capture the images and store to the validation_images folder\")\n",
    "print(\"Press 'q' to stop the enrollment process\")\n",
    "\n",
    "\n",
    "# Define the base path\n",
    "VALIDATION_PATH = os.path.join('application_data', 'validation_images')\n",
    "# Create directory with name\n",
    "new_dir_path = os.path.join(VALIDATION_PATH, name)\n",
    "os.makedirs(new_dir_path, exist_ok=True)\n",
    "\n",
    "# Initialize the webcam\n",
    "import uuid # For generating unique image file names\n",
    "\n",
    "# Function to save the captured image to the specified folder\n",
    "def save_image(image, folder_path, img_name):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    cv2.imwrite(img_path, image)\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(CAM_ID)\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "faces_to_store = []\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face enrollment Process, p for capture, q for quite', frame)\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('p'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/positive'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "           \n",
    "            # At enrollment preocess, preprocess the image before save to file\n",
    "            preprocessed_face = preprocess_wrapper(resized_face).numpy() # Preprocess the image, then convert to numpy array\n",
    "            # Debugging: Check the shape and type of the preprocessed image\n",
    "            print(f\"Preprocessed face shape: {preprocessed_face.shape}, dtype: {preprocessed_face.dtype}\")\n",
    "\n",
    "            # Ensure the preprocessed image is in the correct format for saving\n",
    "            preprocessed_face = (preprocessed_face * 255).astype(np.uint8)\n",
    "\n",
    "            path = os.path.join(VALIDATION_PATH, name)\n",
    "            save_image(preprocessed_face,path , str(uuid.uuid1())+ \".jpg\")\n",
    "\n",
    "            # Append the preprocessed face to the list of faces to store\n",
    "            faces_to_store.append(preprocessed_face)\n",
    "\n",
    "            print(\"Image saved in \", path)\n",
    "        else:\n",
    "            # Show a dialog if no faces are detectedq\n",
    "            print(\"No faces detected, look at the camera and cpature the image again\")\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Save the faces to a compressed file\n",
    "faces_to_store = np.array(faces_to_store)\n",
    "savez_compressed(os.path.join(VALIDATION_PATH, name, 'faces.npz'), faces)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrollment process begins, please look at the camera, rotate your head to the left and right\n",
      "Press 'p' to capture the images and store to the validation_images folder\n",
      "Press 'q' to stop the enrollment process\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "No faces detected, look at the camera and cpature the image again\n",
      "No faces detected, look at the camera and cpature the image again\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n",
      "Preprocessed face shape: (100, 100, 3), dtype: float32\n",
      "Image saved in  application_data\\validation_images\\cta5\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Login process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the validation function which take the input image directly from the webcam, then compare with the images in the validation_images/user_name folder. the parameters of this function are:\n",
    "\n",
    "- `frame`: the input image from the webcam, frame object from OpenCV\n",
    "- `name`: the name of the user that are trying to login\n",
    "- `model`: the model we trained before to generate prediction\n",
    "- `detection_threshold`: the threshold to determine if the input image is a match or not\n",
    "- `verification_threshold`: the threshold to determine the number of matching out of total sample to be considered a valid authentication\n",
    "- `LIMIT_IMAGES_TO_COMPARE`: the number of images in the validation_images/user_name folder to compare with the input image. As we testing, it takes  170ms-200ms to compare one image. So increase this number will increase the time to compare, but also increase the security, otherhand, decrease this number will decrease the time to compare, but also decrease the security.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:12:54.582487Z",
     "start_time": "2024-12-22T05:12:54.574151Z"
    }
   },
   "source": [
    "def verify(frame, name, model, detection_threshold, verification_threshold, LIMIT_IMAGES_TO_COMPARE):\n",
    "    # Detection Threshold: Metric above which the prediction is considered as positive\n",
    "    # Verification Threshold: Proportion of positive detections/ total positive samples\n",
    "\n",
    "    # Create result array\n",
    "    results = []\n",
    "\n",
    "    # Load the input image directly from the Webcam, preprocess it\n",
    "    input_img = preprocess(frame).numpy()\n",
    "\n",
    "    # Process when the name does not exist in the validation_images folder\n",
    "    if not os.path.exists(os.path.join(VALIDATION_PATH, name)):\n",
    "        print(\"The name does not exist in the system\")\n",
    "        return results, False\n",
    "\n",
    "    # Loop through all the images in the validation_images folder (with corresponding name)\n",
    "    path_of_validation_subfolder = os.path.join(VALIDATION_PATH, name)\n",
    "    print(\"Compare with images in folder:\", path_of_validation_subfolder)\n",
    "\n",
    "    # Load the preprocessed faces from the .npz file\n",
    "    data = np.load(os.path.join(path_of_validation_subfolder, 'faces.npz'))\n",
    "\n",
    "    # Get each validation image preprocess function from Part 3\n",
    "    validation_faces = data['arr_0']\n",
    "    for face in validation_faces[:LIMIT_IMAGES_TO_COMPARE]:  # Limit the number of images to compare\n",
    "        # Ensure both images have the same shape\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "        input_img_expanded = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "        # Ensure both images have the same shape\n",
    "        if input_img_expanded.shape != face.shape:\n",
    "            print(f\"Shape mismatch: input_img_expanded shape {input_img_expanded.shape}, face shape {face.shape}\")\n",
    "            continue\n",
    "\n",
    "        # Stack the images along a new dimension\n",
    "        input_pair = np.stack([input_img_expanded, face], axis=1)\n",
    "\n",
    "        # Pass two of these images to the model, and store prediction to the array\n",
    "        result = model.predict(input_pair)\n",
    "        results.append(result)\n",
    "\n",
    "    verification = np.sum(np.array(results) > detection_threshold) / len(results)\n",
    "    verification = verification > verification_threshold\n",
    "\n",
    "    # Return the verification result for further processing\n",
    "    return results, verification"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conbine everythings together, take a single image from Webcam, then call the `verify` function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T05:13:02.132516Z",
     "start_time": "2024-12-22T05:12:54.597702Z"
    }
   },
   "source": [
    "# Ask who is trying to sign in\n",
    "name = input(\"Who are you\")\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(CAM_ID)\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Verification App, press v to capture', frame)\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('v'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/positive'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "        \n",
    "            # Run verification\n",
    "            # with the input image take directly from the webcam, the validation images are taken from the validation_images/name_that_user_input folder\n",
    "            # Here we limit the number of images to compare to 4\n",
    "            results, verification = verify(resized_face, name, model, 0.7, 0.7, 4)\n",
    "\n",
    "            # Arguemnt: 0.7, 0.7, 2\n",
    "            # 0.7: Detection threshold\n",
    "            # 0.7: Verification threshold\n",
    "            # 2: Limit the number of images to compare to when validation\n",
    "            \n",
    "            # print the result\n",
    "            # Print the input frame\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Input Frame\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            #Print out the result\n",
    "            print(\"Verification Result:\", verification)\n",
    "            results = np.array(results).flatten().tolist()\n",
    "            print(\"Model prediction of matching for each validation image:\", results)\n",
    "\n",
    "        else:\n",
    "            # Show a dialog if no faces are detectedq\n",
    "            print(\"No faces detected, look at the camera and cpature the image again\")\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare with images in folder: application_data\\validation_images\\cta5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 36\u001B[0m\n\u001B[0;32m     31\u001B[0m resized_face \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(cropped_face, (\u001B[38;5;241m250\u001B[39m, \u001B[38;5;241m250\u001B[39m))\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Run verification\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# with the input image take directly from the webcam, the validation images are taken from the validation_images/name_that_user_input folder\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# Here we limit the number of images to compare to 4\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m results, verification \u001B[38;5;241m=\u001B[39m \u001B[43mverify\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresized_face\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Arguemnt: 0.7, 0.7, 2\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# 0.7: Detection threshold\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# 0.7: Verification threshold\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# print the result\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Print the input frame\u001B[39;00m\n\u001B[0;32m     45\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(cv2\u001B[38;5;241m.\u001B[39mcvtColor(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB))\n",
      "Cell \u001B[1;32mIn[9], line 31\u001B[0m, in \u001B[0;36mverify\u001B[1;34m(frame, name, model, detection_threshold, verification_threshold, LIMIT_IMAGES_TO_COMPARE)\u001B[0m\n\u001B[0;32m     28\u001B[0m input_img_expanded \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(input_img, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Stack the images along a new dimension\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m input_pair \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minput_img_expanded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mface\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Pass two of these images to the model, and store prediction to the array\u001B[39;00m\n\u001B[0;32m     34\u001B[0m result \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(input_pair)\n",
      "File \u001B[1;32m~\\Desktop\\Biometric_IT4432E\\venv\\Lib\\site-packages\\numpy\\core\\shape_base.py:449\u001B[0m, in \u001B[0;36mstack\u001B[1;34m(arrays, axis, out, dtype, casting)\u001B[0m\n\u001B[0;32m    447\u001B[0m shapes \u001B[38;5;241m=\u001B[39m {arr\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m arrays}\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(shapes) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 449\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall input arrays must have the same shape\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    451\u001B[0m result_ndim \u001B[38;5;241m=\u001B[39m arrays[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    452\u001B[0m axis \u001B[38;5;241m=\u001B[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001B[1;31mValueError\u001B[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

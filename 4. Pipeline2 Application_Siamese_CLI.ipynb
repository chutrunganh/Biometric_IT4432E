{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "In this notebook, we use the Siamese Network we trainied in `Pipeline2 Siamese_Network.ipynb` to build an CLI application that can be used to recognize faces in real time.\n",
    "\n",
    "\n",
    "\n",
    "- Setup verification images/ Enrollment process\n",
    "\n",
    "- Set up Login process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import all dependencies, needed custom functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV\n",
    "import os  # For file operations\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # For plotting graphs\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import savez_compressed\n",
    "from PIL import Image\n",
    "\n",
    "# Import TensorFlow dependencies\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D,  Dense, MaxPool2D, Flatten, Input\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined the Camera ID to use\n",
    "CAM_ID = 0 # Establishing the connection with the IR camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Siamese nodel  need some custom function, Layer that we defined in the building phase.  They are not inside the model itself, so we need to import them. These functions are just copy paste from `Pipeline2 Siamese_Network.ipynb` and `Pipeline2 Data_Preparation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "         super(L1Dist, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self,input_embedding, validation_embedding):\n",
    "        \n",
    "        # Convert inputs to tensors otherwise will meet error: unsupported operand type(s) for -: 'List' and 'List'\n",
    "        input_embedding = tf.convert_to_tensor(input_embedding)\n",
    "        validation_embedding = tf.convert_to_tensor(validation_embedding)\n",
    "        input_embedding = tf.squeeze(input_embedding, axis=0)  # Remove potential first dimension\n",
    "        validation_embedding = tf.squeeze(validation_embedding, axis=0)\n",
    "\n",
    "        # Calculate and return the L1 distance\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model from the saved file ##\n",
    "import tensorflow as tf\n",
    "# Reload the model\n",
    "model = tf.keras.models.load_model('model_saved/fully_siamese_network.h5', \n",
    "                                   custom_objects={'L1Dist': L1Dist, 'BinaryCrossentropy': tf.losses.BinaryCrossentropy},\n",
    "                                   compile=False)\n",
    "# Without complie=false cause Warning: WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(image, kernel_size=(3,3), sigma=0.1):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur to an image using TensorFlow with auto-determined sigma.\n",
    "    \n",
    "    Args:\n",
    "    - image: Input image tensor\n",
    "    - kernel_size: Size of the Gaussian kernel (height, width)\n",
    "    \n",
    "    Returns:\n",
    "    - Smoothed image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the image is a tensor\n",
    "    if not isinstance(image, tf.Tensor):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "    \n",
    "    # Ensure 4D tensor [batch, height, width, channels]\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[tf.newaxis, :, :, :]\n",
    "    \n",
    "    # Create Gaussian kernel for each channel\n",
    "    def create_gaussian_kernel(size, sigma=1.0):\n",
    "        \"\"\"Generate a 2D Gaussian kernel\"\"\"\n",
    "        size = int(size)\n",
    "        x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "        g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "        return g / g.sum()\n",
    "    \n",
    "    # Create kernel\n",
    "    kernel_height, kernel_width = kernel_size\n",
    "    kernel = create_gaussian_kernel(kernel_height, sigma)\n",
    "    \n",
    "    # Expand kernel for all channels\n",
    "    num_channels = image.shape[-1]\n",
    "    kernel_4d = np.expand_dims(kernel, axis=-1)\n",
    "    kernel_4d = np.repeat(kernel_4d, num_channels, axis=-1)\n",
    "    kernel_4d = np.expand_dims(kernel_4d, axis=-1)\n",
    "    \n",
    "    # Convert kernel to float32 tensor\n",
    "    kernel_tensor = tf.convert_to_tensor(kernel_4d, dtype=tf.float32)\n",
    "    \n",
    "    # Apply convolution\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "        input=image, \n",
    "        filter=kernel_tensor, \n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding='SAME'\n",
    "    )\n",
    "    \n",
    "    # Remove batch dimension if it was added\n",
    "    return blurred[0] if blurred.shape[0] == 1 else blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_data):\n",
    "    \"\"\"\n",
    "    Preprocess image data from various input formats into a standardized tensor.\n",
    "    \n",
    "    Args:\n",
    "    input_data: Can be a file path (str), bytes tensor, numpy array, or PIL Image\n",
    "    \n",
    "    Returns:\n",
    "    A preprocessed tensor of shape (100, 100, 3) with values in [0,1]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle PIL Image input\n",
    "        if isinstance(input_data, Image.Image):\n",
    "            input_data = np.array(input_data)\n",
    "        \n",
    "        # Image decoding and initial processing\n",
    "        if isinstance(input_data, (str, bytes)) or (isinstance(input_data, tf.Tensor) and input_data.dtype == tf.string):\n",
    "            # Convert tensor to string if needed\n",
    "            if isinstance(input_data, tf.Tensor):\n",
    "                input_data = input_data.numpy()\n",
    "            if isinstance(input_data, bytes):\n",
    "                input_data = input_data.decode('utf-8')\n",
    "            \n",
    "            # Read and decode the image\n",
    "            byte_image = tf.io.read_file(input_data)\n",
    "            image = tf.image.decode_jpeg(byte_image, channels=3)\n",
    "        else:\n",
    "            # Handle numpy array or TensorFlow tensor input\n",
    "            image = tf.convert_to_tensor(input_data)\n",
    "        \n",
    "        # Convert to float32\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        \n",
    "        # Ensure shape is correct\n",
    "        if len(image.shape) != 3:\n",
    "            raise ValueError(f\"Expected image with 3 dimensions, got shape {image.shape}\")\n",
    "        \n",
    "        # Resize the image\n",
    "        image = tf.image.resize(image, (100, 100))\n",
    "        \n",
    "        # Smooth the image\n",
    "        image = gaussian_blur(image, kernel_size=(3,3), sigma=0.1)\n",
    "        \n",
    "        # Normalize the image\n",
    "        # WIth deep learing, it is ensential to normalize, so   can improve model \n",
    "        # performance by ensuring that input data is within a smaller, consistent range, which can help with stability during training.\n",
    "        image = image / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        '''\n",
    "        However, scaling might make the image look lower quality because of the smaller numerical range (0-1), even though \n",
    "        this does not actually affect its visual structure when used in a deep learning model. This step is not \n",
    "        meant for direct visualization, but rather for preparing data for model input.\n",
    "\n",
    "        If you are trying to visually inspect the image to verify it after scaling, you can:\n",
    "        '''\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        print(f\"Input type: {type(input_data)}\")\n",
    "        if isinstance(input_data, (str, bytes)):\n",
    "            print(f\"Input path: {input_data}\")\n",
    "        raise\n",
    "# Note that our preprocess function return a Tensorflow tensor, not a numpy array, so when need  to  perform image \n",
    "# with OpenCV, we need to convert it to numpy array\n",
    "\n",
    "# Wrap the preprocess function in a tf.py_function to deal with Frame objects in Opencv\n",
    "def preprocess_wrapper(input_data):\n",
    "    \"\"\"Wrapper function to use with tf.py_function if needed\"\"\"\n",
    "    return tf.py_function(preprocess, [input_data], tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overall process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Enrollment**\n",
    "\n",
    "Collect the personal images of that person for verification/enrollment process through webcam. It is similar to when you first choose the sign-in option in Windows Hello, where you need to scan your face images for the first time. These images will be stored to compare with the input image each time you log in to the computer later.\n",
    "\n",
    "2. **Verification/Login**\n",
    "\n",
    "Access webcam -> retrieve input image of user when they want to log in (this input image is processed directly without being stored to any file) -> use this image to verify against a number of positive samples (these positive samples are images already collected as part of our enrollment process). We store the positive samples or called validation images inside the `application_data/verification_images` folder. Each user on the system will have their own subfolder inside the `verification_images` folder.\n",
    "\n",
    "With an input image, loop to compare against all, for example, 50 positive images in `validation_images/user_name` folder -> Our verification function will output 50 predictions. So for example, an input image + one verification image (1 of 50 images in folder) will be compared, and the output will be a number between 0 and 1. We must choose a threshold to determine if the input image is a match or not (**detection threshold**). After that, we get 50 results of matching or not matching. Then we choose a **Verification threshold** to determine the number of matching images out of 50 to be considered a valid authentication. For example, choose the threshold to be 0.8, meaning that if 80% of the 50 images match the input image, then we consider the input image to be a match.\n",
    "\n",
    "\n",
    "![VerificationProcess](assets/images/VerificationProcess.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base directory\n",
    "base_dir = 'application_data'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Create validation images directory\n",
    "validation_dir = os.path.join(base_dir, 'validation_images')\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Enrollment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user to enter their name  to save the verifaction images of that person, then create\n",
    "# a folder with the name of the person in the validation_images folder\n",
    "name = input(\"Enter your name to store your personal verification data to system: \")\n",
    "print(\"enrollment process begins, please look at the camera, rotate your head to the left and right\")\n",
    "print(\"Press 'p' to capture the images and store to the validation_images folder\")\n",
    "print(\"Press 'q' to stop the enrollment process\")\n",
    "\n",
    "\n",
    "# Define the base path\n",
    "VALIDATION_PATH = os.path.join('application_data', 'validation_images')\n",
    "# Create directory with name\n",
    "new_dir_path = os.path.join(VALIDATION_PATH, name)\n",
    "os.makedirs(new_dir_path, exist_ok=True)\n",
    "\n",
    "# Initialize the webcam\n",
    "import uuid # For generating unique image file names\n",
    "\n",
    "# Function to save the captured image to the specified folder\n",
    "def save_image(image, folder_path, img_name):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    cv2.imwrite(img_path, image)\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(CAM_ID)\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face enrollment Process, p for capture, q for quite', frame)\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('p'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/positive'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "           \n",
    "            # At enrollment preocess, preprocess the image before save to file\n",
    "            preprocessed_face = preprocess_wrapper(resized_face).numpy() # Preprocess the image, then convert to numpy array\n",
    "            # Debugging: Check the shape and type of the preprocessed image\n",
    "            print(f\"Preprocessed face shape: {preprocessed_face.shape}, dtype: {preprocessed_face.dtype}\")\n",
    "\n",
    "            # Ensure the preprocessed image is in the correct format for saving\n",
    "            preprocessed_face = (preprocessed_face * 255).astype(np.uint8)\n",
    "\n",
    "            path = os.path.join(VALIDATION_PATH, name)\n",
    "            save_image(preprocessed_face,path , str(uuid.uuid1())+ \".jpg\")\n",
    "            print(\"Image saved in \", path)\n",
    "        else:\n",
    "            # Show a dialog if no faces are detectedq\n",
    "            print(\"No faces detected, look at the camera and cpature the image again\")\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Login process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the validation function which take the input image directly from the webcam, then compare with the images in the validation_images/user_name folder. the parameters of this function are:\n",
    "\n",
    "- `frame`: the input image from the webcam, frame object from OpenCV\n",
    "- `name`: the name of the user that are trying to login\n",
    "- `model`: the model we trained before to generate prediction\n",
    "- `detection_threshold`: the threshold to determine if the input image is a match or not\n",
    "- `verification_threshold`: the threshold to determine the number of matching out of total sample to be considered a valid authentication\n",
    "- `LIMIT_IMAGES_TO_COMPARE`: the number of images in the validation_images/user_name folder to compare with the input image. As we testing, it takes  170ms-200ms to compare one image. So increase this number will increase the time to compare, but also increase the security, otherhand, decrease this number will decrease the time to compare, but also decrease the security.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify (frame, name ,model, detection_threshold, verfication_threshold, LIMIT_IMAGES_TO_COMPARE):\n",
    "    # Detection Threshold: Metric above which the prediction is considered as positive\n",
    "    # Verification Threshold: Proportion of positive detections/ total positive samples\n",
    "\n",
    "    # Example, it te out comes prediction is 0.7, and the detection threshold is 0.5, then the prediction is positive\n",
    "    # If 30 / 50 images pass the detection threshold, then it pass the verification threshold\n",
    "\n",
    "    # Create result array\n",
    "    results = []\n",
    "\n",
    "    # Load the input image directly from the Webcam, preprocess it\n",
    "    input_img = preprocess(frame).numpy()\n",
    "\n",
    "    # Process when the name is not existed in the validation_images folder\n",
    "    if not os.path.exists(os.path.join(VALIDATION_PATH, name)):\n",
    "        print(\"The name does not exist in the system\")\n",
    "        return results, False\n",
    "\n",
    "    # Loop through all the images in the validation_images folder (with crossponding name)\n",
    "    path_of_validation_subfolder = os.path.join(VALIDATION_PATH,name)\n",
    "    print(\"Compare with images in foler:\", path_of_validation_subfolder)\n",
    "\n",
    "    for image in os.listdir(path_of_validation_subfolder)[:LIMIT_IMAGES_TO_COMPARE]: #Limit to only comapre LIMIT_IMAGES_TO_COMPARE images instead of all images inside folder\n",
    "        \n",
    "        # Get each validation image\n",
    "        # preprocess function from Part 3\n",
    "        # The 'name' user input will be used to named the folder in the validation_images folder\n",
    "        \n",
    "        # validation_images  alreadly preprocessed at the enrollment process, so we just need to load the image\n",
    "\n",
    "        # Why need to preprocess at the enrollment process, but not here? -> reduce response time in real time\n",
    "        validation_img = cv2.imread(os.path.join(path_of_validation_subfolder, image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Ensure both images have the same shape and number of channels\n",
    "        if input_img.shape != validation_img.shape:\n",
    "            print(f\"Shape mismatch: input_img shape {input_img.shape}, validation_img shape {validation_img.shape}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Pass two of these images to the model, with  and store preditcion to the array\n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "        \n",
    "\n",
    "    verification = np.sum(np.array(results) > detection_threshold) / len(results)\n",
    "    if verification > verfication_threshold:\n",
    "        verification = True\n",
    "    else:\n",
    "        verification = False\n",
    "\n",
    "    # Return the verification result for futher processing\n",
    "    return results, verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conbine everythings together, take a single image from Webcam, then call the `verify` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ask who is trying to sign in\n",
    "name = input(\"Who are you\")\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(CAM_ID)\n",
    "# Load the Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loop through every frame in the webcam feed\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Verification App, press v to capture', frame)\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('v'):\n",
    "        # Detect face in the frame then crop to 250x250 around the face and save to 'data/positive'\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            cropped_face = frame[y:y+h, x:x+w]\n",
    "            resized_face = cv2.resize(cropped_face, (250, 250))\n",
    "        \n",
    "            # Run verification\n",
    "            # with the input image take directly from the webcam, the validation images are taken from the validation_images/name_that_user_input folder\n",
    "            results, verification = verify(resized_face, name, model, 0.7, 0.7, 4)\n",
    "\n",
    "            # Arguemnt: 0.7, 0.7, 2\n",
    "            # 0.7: Detection threshold\n",
    "            # 0.7: Verification threshold\n",
    "            # 2: Limit the number of images to compare to when validation\n",
    "            \n",
    "            # print the result\n",
    "            # Print the input frame\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Input Frame\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            #Print out the result\n",
    "            print(\"Verification Result:\", verification)\n",
    "            results = np.array(results).flatten().tolist()\n",
    "            print(\"Model prediction of matching for each validation image:\", results)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        else:\n",
    "            # Show a dialog if no faces are detectedq\n",
    "            print(\"No faces detected, look at the camera and cpature the image again\")\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
